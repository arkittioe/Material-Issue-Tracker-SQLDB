# file: data_manager.py

import os
from sqlalchemy import create_engine, func, desc, event
from sqlalchemy.orm import sessionmaker, joinedload
from functools import lru_cache
from datetime import datetime
from models import Base, Project, MIVRecord, MTOItem, MTOConsumption, ActivityLog, MTOProgress, Spool, SpoolItem, \
    SpoolConsumption, SpoolProgress, IsoFileIndex
import numpy as np

"""Improved implementation with edge case handling."""

"""Performance optimization implementation."""
import pandas as pd
import difflib
# data_manager.py (Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ÛŒ ÙØ§ÛŒÙ„)
import logging
import re
from typing import Tuple, List, Dict, Any
import glob
from sqlalchemy.engine import Engine
import time
from config_manager import DB_PATH, DASHBOARD_PASSWORD, ISO_PATH
from ai_engine import Recommender, ShortagePredictor, AnomalyDetector # Ø¨Ø±Ø§ÛŒ ÛŒØ§Ø¯Ú¯ÛŒØ±ÛŒ Ù…Ø§Ø´ÛŒÙ†

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

SPOOL_TYPE_MAPPING = {
    "FLANGE": ("FLG", "FLAN", "FLN"),
    "ELBOW": ("ELB", "ELL", "ELBO"),
    "TEE": ("TEE",),
    "REDUCER": ("RED","REDU","CON","CONN", "ECC"),
    "CAP": ("CAP",),
    "PIPE": ("PIPE", "PIP"),

    # ... Ø´Ù…Ø§ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨ÛŒØ´ØªØ±ÛŒ Ø¨Ù‡ Ø§ÛŒÙ†Ø¬Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯
}
@event.listens_for(Engine, "connect")
def set_sqlite_pragma(dbapi_connection, connection_record):
    """Ø§ÛŒÙ† ØªØ§Ø¨Ø¹ Ù‡Ø± Ø¨Ø§Ø± Ú©Ù‡ ÛŒÚ© Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯ØŒ Ø­Ø§Ù„Øª WAL Ø±Ø§ ÙØ¹Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    cursor = dbapi_connection.cursor()
    try:
        cursor.execute("PRAGMA journal_mode=WAL;")
        cursor.execute("PRAGMA synchronous=NORMAL;")
    finally:
        cursor.close()

class DataManager:
    def __init__(self, db_path=DB_PATH, logger_callback=None):
        """
        Ú©Ù„Ø§Ø³ Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ…Ø§Ù… ØªØ¹Ø§Ù…Ù„Ø§Øª Ø¨Ø§ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡.
        """
        # --- NEW: Ø¯Ø±ÛŒØ§ÙØª Ùˆ ØªÙ†Ø¸ÛŒÙ… Ù„Ø§Ú¯Ø± ---
        self.logger = logger_callback if logger_callback else print

        self.engine = create_engine(
            f"sqlite:///{db_path}",
            echo=False,
            connect_args={'timeout': 15}
        )
        Base.metadata.create_all(self.engine)
        self.Session = sessionmaker(bind=self.engine)

        # --- Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ (Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù„Ø§Ú¯Ø± Ø¬Ø¯ÛŒØ¯) ---
        self.recommender = Recommender()
        self.shortage_predictor = ShortagePredictor()
        self.anomaly_detector = AnomalyDetector()

        # Ø§Ú¯Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø§Ø² Ù‚Ø¨Ù„ Ø¢Ù…ÙˆØ²Ø´ Ù†Ø¯ÛŒØ¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯ØŒ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¢Ù…ÙˆØ²Ø´ Ø¨Ø¯Ù‡
        if not self.recommender.rules:
            self.logger("Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ú¯Ø±...", "info")
            transactions = self.get_all_transactions_for_training(group_by_project=True)
            self.recommender.train(transactions, logger=self.logger)
        else:
            self.recommender.load_model(logger=self.logger)

        if not self.shortage_predictor.models:
            self.logger("Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ø³Ø±ÛŒ (Prophet)...", "info")
            consumption_df = self.get_consumption_history_df()
            self.shortage_predictor.train(consumption_df, logger=self.logger)
        else:
            self.shortage_predictor.load_model(logger=self.logger)

        if self.anomaly_detector.model is None:
            self.logger("Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ...", "info")
            miv_df = self.get_all_mivs_for_training()
            self.anomaly_detector.train(miv_df, logger=self.logger)
        else:
            self.anomaly_detector.load_model(logger=self.logger)

    def get_session(self):
        """ÛŒÚ© Ø³Ø´Ù† Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        return self.Session()

    def log_activity(self, user, action, details="", session=None):
        """Ø«Ø¨Øª Ù„Ø§Ú¯ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ ActivityLog"""
        own_session = False
        if session is None:
            session = self.get_session()
            own_session = True

        try:
            log = ActivityLog(
                user=user,
                action=action,
                details=details,
                timestamp=datetime.now()
            )
            session.add(log)
            if own_session:
                session.commit()
        except Exception as e:
            if own_session:
                session.rollback()
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø«Ø¨Øª Ù„Ø§Ú¯: {e}")
        finally:
            if own_session:
                session.close()

    # --------------------------------------------------------------------
    # Ù…ØªØ¯Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ (CRUD Operations)
    # --------------------------------------------------------------------

    def register_miv_record(self, project_id, form_data, consumption_items, spool_consumption_items=None):
        """
        ÛŒÚ© Ø±Ú©ÙˆØ±Ø¯ MIV Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø«Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ù…ØµØ±Ùâ€ŒÙ‡Ø§ Ø±Ø§ Ù„Ø­Ø§Ø¸ Ú©Ø±Ø¯Ù‡ØŒ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ
        Ùˆ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª Ø¢Ù…Ø§Ø± Ù¾ÛŒØ´Ø±ÙØª Ø®Ø· Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯. (Ù†Ø³Ø®Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡)
        """
        session = self.get_session()
        try:
            # ... (The entire 'try' block remains unchanged) ...
            # 1. Ø³Ø§Ø®Øª Ø±Ú©ÙˆØ±Ø¯ Ø§ØµÙ„ÛŒ MIV
            new_record = MIVRecord(
                project_id=project_id,
                line_no=form_data['Line No'],
                miv_tag=form_data['MIV Tag'],
                location=form_data['Location'],
                status=form_data['Status'],
                comment=form_data.get('Comment', ''),
                registered_for=form_data['Registered For'],
                registered_by=form_data['Registered By'],
                last_updated=datetime.now(),
                is_complete=form_data.get('Complete', False)
            )
            session.add(new_record)
            session.flush()  # Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ new_record.id Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù‚Ø±Ø§Ø± Ú¯ÛŒØ±Ø¯

            # 2. Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÙ…Ø§Ù… Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…ØµØ±ÙÛŒ Ø¨Ø§ ÛŒÚ© Ú©ÙˆØ¦Ø±ÛŒ
            if consumption_items:
                mto_item_ids = [item['mto_item_id'] for item in consumption_items]

                # ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø² Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² (total_qty) Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ù…ÛŒâ€ŒØ³Ø§Ø²ÛŒÙ…
                mto_info_query = session.query(MTOProgress.mto_item_id, MTOProgress.total_qty) \
                    .filter(MTOProgress.mto_item_id.in_(mto_item_ids))

                mto_info_map = {item_id: total_qty for item_id, total_qty in mto_info_query.all()}

                # 3. Ø«Ø¨Øª Ù…ØµØ±Ù Ù…Ø³ØªÙ‚ÛŒÙ… (MTO) Ùˆ Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ
                for item in consumption_items:
                    total_qty_for_item = mto_info_map.get(item['mto_item_id'])

                    # Ø¨Ø±Ø±Ø³ÛŒ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…Ø§Ø¯Ù‡
                    if total_qty_for_item is not None:
                        data_point = {
                            'used_qty': item['used_qty'],
                            'total_qty': total_qty_for_item,
                            'timestamp': new_record.last_updated  # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø²Ù…Ø§Ù† ÛŒÚ©Ø³Ø§Ù† Ø¨Ø±Ø§ÛŒ ØªÙ…Ø§Ù… Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§
                        }
                        self.check_for_anomaly(data_point)

                    # Ø«Ø¨Øª Ø±Ú©ÙˆØ±Ø¯ Ù…ØµØ±Ù Ø¯Ø± Ø¬Ø¯ÙˆÙ„ MTOConsumption
                    session.add(MTOConsumption(
                        mto_item_id=item['mto_item_id'],
                        miv_record_id=new_record.id,
                        used_qty=item['used_qty'],
                        timestamp=new_record.last_updated
                    ))

            # 4. Ø«Ø¨Øª Ù…ØµØ±Ù Ø§Ø² Ø§Ù†Ø¨Ø§Ø± Ø§Ø³Ù¾ÙˆÙ„ (Spool)
            if spool_consumption_items:
                spool_notes = []
                for consumption in spool_consumption_items:
                    spool_item = session.get(SpoolItem, consumption['spool_item_id'])
                    used_qty = consumption['used_qty']

                    if not spool_item:
                        raise ValueError(f"Ø¢ÛŒØªÙ… Ø§Ø³Ù¾ÙˆÙ„ Ø¨Ø§ Ø´Ù†Ø§Ø³Ù‡ {consumption['spool_item_id']} ÛŒØ§ÙØª Ù†Ø´Ø¯.")

                    is_pipe = "PIPE" in (spool_item.component_type or "").upper()
                    if is_pipe:
                        if (spool_item.length or 0) < used_qty:
                            raise ValueError(f"Ø·ÙˆÙ„ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒÙ¾ Ø¯Ø± Ø§Ø³Ù¾ÙˆÙ„ {spool_item.spool.spool_id} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
                        spool_item.length -= used_qty
                    else:
                        if (spool_item.qty_available or 0) < used_qty:
                            raise ValueError(
                                f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ {spool_item.component_type} Ø¯Ø± Ø§Ø³Ù¾ÙˆÙ„ {spool_item.spool.spool_id} ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
                        spool_item.qty_available -= used_qty

                    session.add(SpoolConsumption(
                        spool_item_id=spool_item.id,
                        spool_id=spool_item.spool.id,
                        miv_record_id=new_record.id,
                        used_qty=used_qty,
                        timestamp=new_record.last_updated
                    ))

                    unit = "m" if is_pipe else "Ø¹Ø¯Ø¯"
                    spool_notes.append(
                        f"{used_qty:.2f} {unit} Ø§Ø² {spool_item.component_type} (Ø§Ø³Ù¾ÙˆÙ„: {spool_item.spool.spool_id})")

                if spool_notes:
                    # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ú©Ø§Ù…Ù†Øª Ø±Ú©ÙˆØ±Ø¯ MIV Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…ØµØ±Ù Ø§Ø³Ù¾ÙˆÙ„
                    final_comment = (new_record.comment or "")
                    if final_comment:
                        final_comment += " | "
                    final_comment += "Ù…ØµØ±Ù Ø§Ø³Ù¾ÙˆÙ„: " + ", ".join(spool_notes)
                    new_record.comment = final_comment

            # 5. Ù†Ù‡Ø§ÛŒÛŒâ€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø«Ø¨Øª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            session.commit()

            # 6. Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø¢Ù…Ø§Ø± Ù¾ÛŒØ´Ø±ÙØª Ø¨Ø±Ø§ÛŒ Ø®Ø· Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ù¾Ø³ Ø§Ø² Ø«Ø¨Øª Ù…ÙˆÙÙ‚
            self.rebuild_mto_progress_for_line(project_id, form_data['Line No'])

            # 7. Ø«Ø¨Øª Ù„Ø§Ú¯ ÙØ¹Ø§Ù„ÛŒØª
            self.log_activity(
                user=form_data['Registered By'], action="REGISTER_MIV",
                details=f"MIV Tag '{form_data['MIV Tag']}' for Line '{form_data['Line No']}'",
            )
            return True, "Ø±Ú©ÙˆØ±Ø¯ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø«Ø¨Øª Ø´Ø¯."

        except Exception as e:
            session.rollback()
            import traceback
            # CHANGE: Replaced logging.error with self.logger
            self.logger(f"Ø®Ø·Ø§ Ø¯Ø± Ø«Ø¨Øª Ø±Ú©ÙˆØ±Ø¯: {e}\n{traceback.format_exc()}", "error")
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø«Ø¨Øª Ø±Ú©ÙˆØ±Ø¯: {e}"
        finally:
            session.close()

    def update_miv_items(self, miv_record_id, updated_items, updated_spool_items, user="system"):
        session = self.get_session()
        try:
            record = session.get(MIVRecord, miv_record_id)
            if not record:
                return False, f"MIV Ø¨Ø§ Ø´Ù†Ø§Ø³Ù‡ {miv_record_id} ÛŒØ§ÙØª Ù†Ø´Ø¯."

            project_id = record.project_id
            line_no = record.line_no

            # --- Ù…Ø¯ÛŒØ±ÛŒØª Ù…ØµØ±Ù Ø§Ø³Ù¾ÙˆÙ„ ---
            # 1. Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù…ÙˆØ¬ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ø§Ø³Ù¾ÙˆÙ„ Ø¨Ù‡ Ø§Ù†Ø¨Ø§Ø±
            old_spool_consumptions = session.query(SpoolConsumption).filter(
                SpoolConsumption.miv_record_id == miv_record_id).all()
            for old_c in old_spool_consumptions:
                spool_item = session.get(SpoolItem, old_c.spool_item_id)
                if spool_item:
                    is_pipe = "PIPE" in (spool_item.component_type or "").upper()
                    if is_pipe:
                        spool_item.length = (spool_item.length or 0) + old_c.used_qty
                    else:
                        spool_item.qty_available = (spool_item.qty_available or 0) + old_c.used_qty

            # 2. Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù…ØµØ±Ù Ù‚Ø¯ÛŒÙ…ÛŒ (Ù‡Ù… MTO Ùˆ Ù‡Ù… Spool)
            session.query(MTOConsumption).filter(MTOConsumption.miv_record_id == miv_record_id).delete()
            session.query(SpoolConsumption).filter(SpoolConsumption.miv_record_id == miv_record_id).delete()
            session.flush()

            # --- Ø«Ø¨Øª Ù…ØµØ±Ùâ€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ ---
            # 3. Ø«Ø¨Øª Ù…ØµØ±Ù Ù…Ø³ØªÙ‚ÛŒÙ… MTO
            for item in updated_items:
                session.add(MTOConsumption(
                    mto_item_id=item["mto_item_id"],
                    miv_record_id=miv_record_id,
                    used_qty=item["used_qty"],
                    timestamp=datetime.now()
                ))

            # 4. Ø«Ø¨Øª Ù…ØµØ±Ù Ø¬Ø¯ÛŒØ¯ Ø§Ø³Ù¾ÙˆÙ„
            spool_notes = []
            if updated_spool_items:
                for s_item in updated_spool_items:
                    spool_item = session.get(SpoolItem, s_item['spool_item_id'])
                    used_qty = s_item['used_qty']

                    if not spool_item:
                        raise ValueError(f"Ø¢ÛŒØªÙ… Ø§Ø³Ù¾ÙˆÙ„ Ø¨Ø§ Ø´Ù†Ø§Ø³Ù‡ {s_item['spool_item_id']} ÛŒØ§ÙØª Ù†Ø´Ø¯.")

                    is_pipe = "PIPE" in (spool_item.component_type or "").upper()

                    if is_pipe:
                        if (spool_item.length or 0) < used_qty:
                            raise ValueError(f"Ø·ÙˆÙ„ Ù…ÙˆØ¬ÙˆØ¯ Ù¾Ø§ÛŒÙ¾ Ø¯Ø± Ø§Ø³Ù¾ÙˆÙ„ {spool_item.spool.spool_id} Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª.")
                        spool_item.length -= used_qty
                    else:
                        if (spool_item.qty_available or 0) < used_qty:
                            raise ValueError(
                                f"Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¢ÛŒØªÙ… {spool_item.component_type} Ø¯Ø± Ø§Ø³Ù¾ÙˆÙ„ {spool_item.spool.spool_id} Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª.")
                        spool_item.qty_available -= used_qty

                    session.add(SpoolConsumption(
                        spool_item_id=spool_item.id,
                        spool_id=spool_item.spool.id,
                        miv_record_id=miv_record_id,
                        used_qty=used_qty,
                        timestamp=datetime.now()
                    ))
                    # Ø³Ø§Ø®Øª Note
                    unit = "mm" if is_pipe else "Ø¹Ø¯Ø¯"
                    spool_notes.append(
                        f"{used_qty:.1f} {unit} Ø§Ø² {spool_item.component_type} (Ø§Ø³Ù¾ÙˆÙ„: {spool_item.spool.spool_id})")

            # 5. (Ù…Ù‡Ù…) Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ú©Ø§Ù…Ù„ Ø¢Ù…Ø§Ø± Ø®Ø· Ø¨Ø¹Ø¯ Ø§Ø² ØªÙ…Ø§Ù… ØªØºÛŒÛŒØ±Ø§Øª
            session.commit()
            self.rebuild_mto_progress_for_line(project_id, line_no)

            self.log_activity(
                user=user,
                action="UPDATE_MIV_ITEMS",
                details=f"Consumption items updated for MIV {miv_record_id}",
            )
            return True, "Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…ØµØ±ÙÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯Ù†Ø¯."

        except Exception as e:
            session.rollback()
            import traceback
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ MIV {miv_record_id}: {e}\n{traceback.format_exc()}")
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ MIV: {e}"
        finally:
            session.close()

    def delete_miv_record(self, record_id, user="system"):
        """
        ÛŒÚ© Ø±Ú©ÙˆØ±Ø¯ MIV Ùˆ ØªÙ…Ø§Ù… Ù…ØµØ±Ùâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø¢Ù† (MTO Ùˆ Spool) Ø±Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ù…ØµØ±Ù Ø´Ø¯Ù‡ Ø§Ø² Ø§Ø³Ù¾ÙˆÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø§Ù†Ø¨Ø§Ø± Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
        Ø³Ù¾Ø³ Ø¬Ø¯ÙˆÙ„ MTOProgress Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù† Ø®Ø· Ø¨Ù‡â€ŒØ·ÙˆØ± Ú©Ø§Ù…Ù„ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        session = self.get_session()
        try:
            # Û±. Ø±Ú©ÙˆØ±Ø¯ Ø§ØµÙ„ÛŒ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†
            record = session.get(MIVRecord, record_id)
            if not record:
                return False, "Ø±Ú©ÙˆØ±Ø¯ ÛŒØ§ÙØª Ù†Ø´Ø¯."

            project_id = record.project_id
            line_no = record.line_no
            miv_tag = record.miv_tag

            # Û². (Ù…Ù‡Ù…) Ù…ÙˆØ¬ÙˆØ¯ÛŒâ€ŒÙ‡Ø§ÛŒ Ù…ØµØ±ÙÛŒ Ø§Ø³Ù¾ÙˆÙ„ Ø±Ø§ Ø¨Ù‡ Ø§Ù†Ø¨Ø§Ø± Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†
            spool_consumptions = session.query(SpoolConsumption).filter(SpoolConsumption.miv_record_id == record_id).all()
            for consumption in spool_consumptions:
                spool_item = session.get(SpoolItem, consumption.spool_item_id)
                if spool_item:
                    is_pipe = "PIPE" in (spool_item.component_type or "").upper()
                    if is_pipe:
                        spool_item.length = (spool_item.length or 0) + consumption.used_qty
                    else:
                        spool_item.qty_available = (spool_item.qty_available or 0) + consumption.used_qty

            # Û³. ØªÙ…Ø§Ù… Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù…ØµØ±ÙÛŒ Ù…Ø±ØªØ¨Ø· (MTO Ùˆ Spool) Ø±Ø§ Ø­Ø°Ù Ú©Ù†
            session.query(MTOConsumption).filter(MTOConsumption.miv_record_id == record_id).delete()
            session.query(SpoolConsumption).filter(SpoolConsumption.miv_record_id == record_id).delete()

            # Û´. Ø®ÙˆØ¯ Ø±Ú©ÙˆØ±Ø¯ MIV Ø±Ø§ Ø­Ø°Ù Ú©Ù†
            session.delete(record)
            session.commit()

            # Ûµ. (Ù…Ù‡Ù…) Ø¢Ù…Ø§Ø± Ù¾ÛŒØ´Ø±ÙØª Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ø®Ø· Ø§Ø² Ù†Ùˆ Ø¨Ø³Ø§Ø²
            self.rebuild_mto_progress_for_line(project_id, line_no)

            # Û¶. Ø«Ø¨Øª Ù„Ø§Ú¯
            self.log_activity(
                user=user,
                action="DELETE_MIV",
                details=f"Deleted MIV Record ID {record_id} (Tag: {miv_tag}) for line {line_no}"
            )

            return True, "Ø±Ú©ÙˆØ±Ø¯ Ùˆ Ù…ØµØ±Ùâ€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø­Ø°Ù Ø´Ø¯Ù†Ø¯."

        except Exception as e:
# FIXME: Optimize this section for better performance
            session.rollback()
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯ MIV Ø¨Ø§ Ø´Ù†Ø§Ø³Ù‡ {record_id}: {e}")
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯: {e}"
        finally:
            session.close()

    def rebuild_mto_progress_for_line(self, project_id, line_no):
        """
        (Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡)
        Ø¢Ù…Ø§Ø± Ù¾ÛŒØ´Ø±ÙØª ØªÙ…Ø§Ù… Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ MTO ÛŒÚ© Ø®Ø· Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ÛŒÚ© Ú©ÙˆØ¦Ø±ÛŒ Ø¬Ø§Ù…Ø¹ Ùˆ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        session = self.get_session()
        try:
            # Ú¯Ø§Ù… Û±: ØªÙ…Ø§Ù… MTO Item Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø±Ø§ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ù…ØµØ±Ù Ù…Ø³ØªÙ‚ÛŒÙ… (direct_used) ÙˆØ§Ú©Ø´ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            # Ø§Ø² left join Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ØªØ§ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù…ØµØ±Ù Ù‡Ù… Ù„ÛŒØ³Øª Ø´ÙˆÙ†Ø¯.
            base_query = (
                session.query(
                    MTOItem,
                    func.coalesce(func.sum(MTOConsumption.used_qty), 0.0).label("direct_used")
                )
                .outerjoin(MTOConsumption, MTOItem.id == MTOConsumption.mto_item_id)
                .filter(MTOItem.project_id == project_id, MTOItem.line_no == line_no)
                .group_by(MTOItem.id)
            )

            mto_items_with_direct_usage = base_query.all()
            if not mto_items_with_direct_usage:
                return  # Ø§Ú¯Ø± Ø®Ø· Ù‡ÛŒÚ† Ø¢ÛŒØªÙ…ÛŒ Ù†Ø¯Ø§Ø´ØªØŒ Ø®Ø§Ø±Ø¬ Ø´Ùˆ

            # Ú¯Ø§Ù… Û²: ØªÙ…Ø§Ù… Ù…ØµØ±Ùâ€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾ÙˆÙ„ Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø§ÛŒÙ† Ø®Ø· Ø±Ø§ ÛŒÚ©â€ŒØ¬Ø§ ÙˆØ§Ú©Ø´ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            spool_consumptions_in_line = (
                session.query(
                    func.upper(SpoolItem.component_type).label("spool_type"),
                    SpoolItem.p1_bore,
                    func.sum(SpoolConsumption.used_qty).label("total_spool_used")
                )
                .join(MIVRecord, SpoolConsumption.miv_record_id == MIVRecord.id)
                .join(SpoolItem, SpoolConsumption.spool_item_id == SpoolItem.id)
                .filter(MIVRecord.project_id == project_id, MIVRecord.line_no == line_no)
                .group_by("spool_type", SpoolItem.p1_bore)
                .all()
            )

            # Ú¯Ø§Ù… Û³: Ù…ØµØ±Ù Ø§Ø³Ù¾ÙˆÙ„ Ø±Ø§ Ø¯Ø± ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø³Ø±ÛŒØ¹ Ø¢Ù…Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            spool_usage_map = {
                (usage.spool_type, usage.p1_bore): usage.total_spool_used
                for usage in spool_consumptions_in_line
            }

            progress_updates = []
            mto_item_ids_in_line = [item.id for item, _ in mto_items_with_direct_usage]

            # Ú¯Ø§Ù… Û´: Ø±ÙˆÛŒ Ù†ØªØ§ÛŒØ¬ ÙˆØ§Ú©Ø´ÛŒ Ø´Ø¯Ù‡ Ø­Ø±Ú©Øª Ú©Ø±Ø¯Ù‡ Ùˆ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø±Ø§ Ø¯Ø± Ù¾Ø§ÛŒØªÙˆÙ† Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ….
            for mto_item, direct_used in mto_items_with_direct_usage:
# OPTIMIZE: Use caching for repeated calls
                is_pipe = mto_item.item_type and 'pipe' in mto_item.item_type.lower()
                total_required = mto_item.length_m if is_pipe else mto_item.quantity

                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…ØµØ±Ù Ø§Ø³Ù¾ÙˆÙ„ Ù…Ø¹Ø§Ø¯Ù„
                mto_type_upper = str(mto_item.item_type).upper().strip()
                spool_equivalents = {mto_type_upper}
                for key, aliases in SPOOL_TYPE_MAPPING.items():
                    if mto_type_upper == key or mto_type_upper in aliases:
                        spool_equivalents.update([key] + list(aliases))
                        break

                spool_used = 0
                for eq_type in spool_equivalents:
                    spool_used += spool_usage_map.get((eq_type, mto_item.p1_bore_in), 0)

                total_used = (direct_used or 0) + spool_used
                remaining = max(0, (total_required or 0) - total_used)

                # Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª Ú¯Ø±ÙˆÙ‡ÛŒ (bulk update) Ø¢Ù…Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
                progress_updates.append({
                    'mto_item_id': mto_item.id,
                    'project_id': project_id,
                    'line_no': line_no,
                    'item_code': mto_item.item_code,
                    'description': mto_item.description,
                    'unit': mto_item.unit,
                    'total_qty': round(total_required or 0, 2),
                    'used_qty': round(total_used, 2),
                    'remaining_qty': round(remaining, 2),
                    'last_updated': datetime.now()
                })

            # Ú¯Ø§Ù… Ûµ: ØªÙ…Ø§Ù… Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ù¾ÛŒØ´Ø±ÙØª Ø±Ø§ Ø­Ø°Ù Ú©Ø±Ø¯Ù‡ Ùˆ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ ÛŒÚ©â€ŒØ¬Ø§ Ø¯Ø±Ø¬ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            session.query(MTOProgress).filter(
                MTOProgress.mto_item_id.in_(mto_item_ids_in_line)
            ).delete(synchronize_session=False)

            if progress_updates:
                session.bulk_insert_mappings(MTOProgress, progress_updates)

            session.commit()
        except Exception as e:
            session.rollback()
            import traceback
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± rebuild_mto_progress_for_line (Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡): {e}\n{traceback.format_exc()}")
        finally:
            session.close()

    def get_consumptions_for_miv(self, miv_record_id):
        """
        ØªÙ…Ø§Ù… Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…ØµØ±ÙÛŒ Ø«Ø¨Øªâ€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© MIV Ø®Ø§Øµ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
        Ø®Ø±ÙˆØ¬ÛŒ ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø³Øª Ú©Ù‡ Ú©Ù„ÛŒØ¯ Ø¢Ù† mto_item_id Ùˆ Ù…Ù‚Ø¯Ø§Ø± Ø¢Ù† used_qty Ø§Ø³Øª.
        """
        session = self.get_session()
        try:
            consumptions = session.query(MTOConsumption).filter(
                MTOConsumption.miv_record_id == miv_record_id
            ).all()
            # ØªØ¨Ø¯ÛŒÙ„ Ù„ÛŒØ³Øª Ø¨Ù‡ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø³Ø±ÛŒØ¹â€ŒØªØ±
            return {item.mto_item_id: item.used_qty for item in consumptions}
        except Exception as e:
            logging.error(f"Error fetching consumptions for MIV {miv_record_id}: {e}")
            return {}
        finally:
            session.close()

    # --------------------------------------------------------------------
    # Ù…ØªØ¯Ù‡Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ
    # --------------------------------------------------------------------

    def is_duplicate_miv_tag(self, miv_tag, project_id):
        """Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ ÛŒÚ© MIV Tag Ø¯Ø± ÛŒÚ© Ù¾Ø±ÙˆÚ˜Ù‡ Ø®Ø§Øµ ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±."""
        session = self.get_session()
        try:
            exists = session.query(MIVRecord.id).filter(
                MIVRecord.project_id == project_id,
                MIVRecord.miv_tag == miv_tag
            ).first()
            return exists is not None
        finally:
            session.close()

    def get_line_no_suggestions(self, typed_text: str, top_n: int = 15) -> List[Dict[str, Any]]:
        """
        (Ù†Ø³Ø®Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² LIKE)
        Ø¯Ø± ØªÙ…Ø§Ù… Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ Ø¬Ø³ØªØ¬Ùˆ Ú©Ø±Ø¯Ù‡ Ùˆ Ø´Ù…Ø§Ø±Ù‡ Ø®Ø·â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø§Ø¨Ù‡ Ø±Ø§ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ù†Ø§Ù… Ù¾Ø±ÙˆÚ˜Ù‡ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
        Ø§ÛŒÙ† Ø¬Ø³ØªØ¬Ùˆ Ù…Ø³ØªÙ‚ÛŒÙ…Ø§Ù‹ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ùˆ Ø¨Ø§ Ø³Ø±Ø¹Øª Ø¨Ø§Ù„Ø§ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        """
        if not typed_text or len(typed_text) < 2:
            return []

        session = self.get_session()
        try:
            # Ø³Ø§Ø®Øª Ø¹Ø¨Ø§Ø±Øª Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ø§Ù¾Ø±Ø§ØªÙˆØ± LIKE
            search_term = f"%{typed_text}%"

            # Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡ Ú©Ù‡ ÙÛŒÙ„ØªØ± Ø±Ø§ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
            query = (
                session.query(
                    MTOItem.line_no,
                    Project.name,
                    Project.id
                )
                .join(Project, MTOItem.project_id == Project.id)
                .filter(MTOItem.line_no.ilike(search_term))  # ilike Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ ØºÛŒØ±Ø­Ø³Ø§Ø³ Ø¨Ù‡ Ø­Ø±ÙˆÙ
                .distinct()
                .limit(top_n)
            )

            results = query.all()

            # ØªØ¨Ø¯ÛŒÙ„ Ù†ØªØ§ÛŒØ¬ Ø¨Ù‡ ÙØ±Ù…Øª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² UI
            suggestions = [
                {
                    'display': f"{line_no}  ({project_name})",
                    'line_no': line_no,
                    'project_name': project_name,
                    'project_id': project_id
                }
                for line_no, project_name, project_id in results
            ]
            return suggestions

        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø³Ø±Ø§Ø³Ø±ÛŒ Ø´Ù…Ø§Ø±Ù‡ Ø®Ø· (Ø¨Ù‡ÛŒÙ†Ù‡ Ø´Ø¯Ù‡): {e}")
            return []
        finally:
            session.close()

    def search_miv_by_line_no(self, project_id, line_no):
        """ØªÙ…Ø§Ù… Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ MIV Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ ÛŒÚ© Ø´Ù…Ø§Ø±Ù‡ Ø®Ø· Ø¯Ø± ÛŒÚ© Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
        session = self.get_session()
        try:
            records = session.query(MIVRecord).filter(
                MIVRecord.project_id == project_id,
                MIVRecord.line_no == line_no
            ).all()
            return records
        finally:
            session.close()

    def get_mto_item_by_id(self, mto_item_id: int):
        """ÛŒÚ© Ø¢ÛŒØªÙ… MTO Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ù†Ø§Ø³Ù‡ Ø§ØµÙ„ÛŒ Ø¢Ù† Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
        session = self.get_session()
        try:
            # Ø§Ø² session.get Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ù…Ø³ØªÙ‚ÛŒÙ… Ùˆ Ø³Ø±ÛŒØ¹ Ø¨Ù‡ Ø¢ÛŒØªÙ… Ø¨Ø§ ID Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            return session.get(MTOItem, mto_item_id)
        except Exception as e:
            logging.error(f"Error fetching MTO item with id {mto_item_id}: {e}")
            return None
        finally:
            session.close()

    # --------------------------------------------------------------------
    # Ù…ØªØ¯Ù‡Ø§ÛŒ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ (Viewers & Tables)
    # --------------------------------------------------------------------

    def get_miv_data(self, project_id, mode='all', line_no=None, last_n=None):
        """Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ MIV Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø­Ø§Ù„Øªâ€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
        session = self.get_session()
        try:
            query = session.query(MIVRecord).filter(MIVRecord.project_id == project_id)

            if mode == 'complete':
                query = query.filter(MIVRecord.is_complete == True)
            elif mode == 'incomplete':
                query = query.filter(MIVRecord.is_complete == False)

            if line_no:
                query = query.filter(MIVRecord.line_no == line_no)

            if last_n:
                query = query.order_by(MIVRecord.last_updated.desc()).limit(last_n)

            return query.all()
        finally:
            session.close()

    def get_mto_items_for_line(self, project_id, line_no):
        """ØªÙ…Ø§Ù… Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ MTO Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ø´Ù…Ø§Ø±Ù‡ Ø®Ø· Ø®Ø§Øµ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
        session = self.get_session()
        try:
            # Ø§ÛŒÙ† Ú©ÙˆØ¦Ø±ÛŒ ØªÙ…Ø§Ù… Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ù¾Ù†Ø¬Ø±Ù‡ Ù…ØµØ±Ù Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯
            items = session.query(MTOItem).filter(
                MTOItem.project_id == project_id,
                MTOItem.line_no == line_no
            ).all()
            return items
        finally:
            session.close()

    def get_all_projects(self):
        """Ù„ÛŒØ³Øª Ù‡Ù…Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
        session = self.get_session()
        try:
            return session.query(Project).order_by(Project.name).all()
        finally:
            session.close()

    def get_project_by_name(self, name):
        """Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù… Ø¯Ù‚ÛŒÙ‚ Ø¢Ù† Ø¬Ø³ØªØ¬Ùˆ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        session = self.get_session()
        try:
            return session.query(Project).filter(Project.name == name).first()
        finally:
            session.close()

    # --------------------------------------------------------------------
    # Ù…ØªØ¯Ù‡Ø§ÛŒ Ú¯Ø²Ø§Ø±Ø´â€ŒÚ¯ÛŒØ±ÛŒ (Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ùˆ Ú¯Ø²Ø§Ø±Ø´â€ŒÙ‡Ø§)
    # --------------------------------------------------------------------
    # Ù…ØªØ¯Ù‡Ø§ÛŒ get_project_progress, get_line_progress Ùˆ generate_project_report Ú©Ù‡ Ù‚Ø¨Ù„Ø§Ù‹ Ù†ÙˆØ´ØªÙ‡â€ŒØ§ÛŒØ¯
    # Ø¯Ø± Ø§ÛŒÙ†Ø¬Ø§ Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ¯ÛŒØ±Ù†Ø¯ Ùˆ Ú©Ø§Ù…Ù„ Ù‡Ø³ØªÙ†Ø¯.
    @lru_cache(maxsize=128)
    def get_project_progress(self, project_id, default_diameter=1):
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ÛŒØ´Ø±ÙØª Ú©Ù„ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³
        - ÙˆØ²Ù† Ù‡Ø± Ø®Ø· = (Ù…Ø¬Ù…ÙˆØ¹ LENGTH(M) + QUANTITY) Ã— Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù‚Ø·Ø± Ù¾Ø§ÛŒÙ¾ Ø¯Ø± Ø¢Ù† Ø®Ø·
        - Ø¯Ø±ØµØ¯ Ù¾ÛŒØ´Ø±ÙØª = ÙˆØ²Ù† Ø§Ù†Ø¬Ø§Ù…â€ŒØ´Ø¯Ù‡ / ÙˆØ²Ù† Ú©Ù„ Ã— 100
        """
        from models import MTOItem, MTOConsumption, MIVRecord

        session = self.get_session()
        try:
            # Ú¯Ø±ÙØªÙ† ØªÙ…Ø§Ù… Ø´Ù…Ø§Ø±Ù‡ Ø®Ø·ÙˆØ· Ù¾Ø±ÙˆÚ˜Ù‡
            lines = session.query(MTOItem.line_no).filter(MTOItem.project_id == project_id).distinct().all()
            if not lines:
                return {"total_lines": 0, "total_weight": 0, "done_weight": 0, "percentage": 0}

            total_weight = 0
            done_weight = 0

            for (line_no,) in lines:
                # Ú¯Ø±ÙØªÙ† ØªÙ…Ø§Ù… Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ† Ø®Ø·
                items = session.query(MTOItem).filter(
                    MTOItem.project_id == project_id,
                    MTOItem.line_no == line_no
                ).all()

                if not items:
                    continue

                # Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ù‚Ø·Ø± Ù¾Ø§ÛŒÙ¾ Ø¯Ø± Ø§ÛŒÙ† Ø®Ø· (Ø¯Ø± MTOItem Ù¾ÛŒÚ©Ø³Ù„ÛŒ Ù†ÛŒØ³Øª ÙˆÙ„ÛŒ Ù…ÛŒØ´Ù‡ Ø¨Ù‡ item_type Ø§Ø³ØªÙ†Ø§Ø¯ Ú©Ø±Ø¯)
                max_diameter = default_diameter
                for item in items:
                    if item.item_type and "pipe" in item.item_type.lower():
                        try:
                            # ÙØ±Ø¶: Ø·ÙˆÙ„ ÛŒØ§ Ù‚Ø·Ø± Ù¾Ø§ÛŒÙ¾ Ø¯Ø± description ÛŒØ§ unit Ø°Ø®ÛŒØ±Ù‡ Ù†Ø´Ø¯Ù‡ØŒ ÙØ¹Ù„Ø§Ù‹ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ù…ÛŒâ€ŒØ²Ù†ÛŒÙ…
                            pass
                        except:
                            pass

                # Ù…Ø¬Ù…ÙˆØ¹ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø·ÙˆÙ„ Ùˆ ØªØ¹Ø¯Ø§Ø¯
                length_sum = sum(item.length_m or 0 for item in items)
                qty_sum = sum(item.quantity or 0 for item in items)
                qty_sum_effective = length_sum + qty_sum

                line_weight = qty_sum_effective * max_diameter
                total_weight += line_weight

                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ØµØ±Ùâ€ŒØ´Ø¯Ù‡
                used_qty = (
                               session.query(func.coalesce(func.sum(MTOConsumption.used_qty), 0))
                               .join(MTOItem, MTOConsumption.mto_item_id == MTOItem.id)
                               .filter(MTOItem.project_id == project_id, MTOItem.line_no == line_no)
                               .scalar()
                           ) or 0

                done_weight += used_qty * max_diameter

            percentage = round((done_weight / total_weight * 100), 2) if total_weight > 0 else 0

            return {
                "total_lines": len(lines),
                "total_weight": total_weight,
                "done_weight": done_weight,
                "percentage": percentage
            }

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ÛŒØ´Ø±ÙØª Ù¾Ø±ÙˆÚ˜Ù‡: {e}")
            return {"total_lines": 0, "total_weight": 0, "done_weight": 0, "percentage": 0}
        finally:
            session.close()

    @lru_cache(maxsize=256)
    def get_line_progress(self, project_id, line_no, readonly=True):  # ğŸ”¹ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ default_diameter Ù†ÛŒØ³Øª
        """
        Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ÛŒØ´Ø±ÙØª ÛŒÚ© Ø®Ø· Ø®Ø§Øµ Ø¯Ø± Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ MTOProgress.
        """
        session = self.get_session()
        try:
            # Ø¬Ù…Ø¹ Ú©Ù„ Ùˆ Ù…ØµØ±Ù Ø´Ø¯Ù‡ Ø§Ø² Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ MTOProgress
            # Ù…Ø§ ÙØ±Ø¶ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ù‡Ø± Ø¢ÛŒØªÙ… ÙˆØ²Ù† ÛŒÚ©Ø³Ø§Ù†ÛŒ Ø¯Ø§Ø±Ø¯. Ø§Ú¯Ø± ÙˆØ²Ù†â€ŒØ¯Ù‡ÛŒ Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ± Ù†ÛŒØ§Ø² Ø¨ÙˆØ¯ØŒ Ù…Ù†Ø·Ù‚ ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
            query_result = session.query(
                func.sum(MTOProgress.total_qty),
                func.sum(MTOProgress.used_qty)
            ).filter(
                MTOProgress.project_id == project_id,
                MTOProgress.line_no == line_no
            ).first()

            total_weight, done_weight = query_result
            total_weight = total_weight or 0
            done_weight = done_weight or 0

            if total_weight == 0 and not readonly:
                # Ø§Ú¯Ø± Ø®Ø· Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¯Ø± MTOProgress Ù†Ø¯Ø§Ø´ØªØŒ ÛŒÚ© Ø¨Ø§Ø± Ø¢Ù† Ø±Ø§ Ø¨Ø³Ø§Ø²
                self.initialize_mto_progress_for_line(project_id, line_no)
                query_result = session.query(
                    func.sum(MTOProgress.total_qty),
                    func.sum(MTOProgress.used_qty)
                ).filter(
                    MTOProgress.project_id == project_id,
                    MTOProgress.line_no == line_no
                ).first()
                total_weight, done_weight = query_result
                total_weight = total_weight or 0
                done_weight = done_weight or 0

            percentage = round((done_weight / total_weight * 100), 2) if total_weight > 0 else 0

            return {
                "line_no": line_no,
                "total_weight": total_weight,
                "done_weight": done_weight,
                "percentage": percentage
            }

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù¾ÛŒØ´Ø±ÙØª Ø®Ø· {line_no}: {e}")
            return {"line_no": line_no, "total_weight": 0, "done_weight": 0, "percentage": 0}
        finally:
            session.close()

    def generate_project_report(self, project_id):
        """
        ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ù¾ÛŒØ´Ø±ÙØª Ù¾Ø±ÙˆÚ˜Ù‡
        Ø´Ø§Ù…Ù„ Ø¯Ø±ØµØ¯ Ù¾ÛŒØ´Ø±ÙØª Ú©Ù„ÛŒ Ùˆ Ø¬Ø²Ø¦ÛŒØ§Øª Ù‡Ø± Ø®Ø·
        """
        report = {
            "project_id": project_id,
            "summary": self.get_project_progress(project_id),
            "lines": []
        }

        # Ú¯Ø±ÙØªÙ† Ø´Ù…Ø§Ø±Ù‡ ØªÙ…Ø§Ù… Ø®Ø·ÙˆØ· Ù¾Ø±ÙˆÚ˜Ù‡
        from models import MTOItem
        session = self.get_session()
        try:
            lines = (
                session.query(MTOItem.line_no)
                .filter(MTOItem.project_id == project_id)
                .distinct()
                .all()
            )
            for (line_no,) in lines:
                line_progress = self.get_line_progress(project_id, line_no)
                report["lines"].append(line_progress)

            # Ø«Ø¨Øª Ú¯Ø²Ø§Ø±Ø´ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÙØ¹Ø§Ù„ÛŒØª
            self.log_activity(
                user="system",  # ÛŒØ§ Ù†Ø§Ù… Ú©Ø§Ø±Ø¨Ø±ÛŒ Ú©Ù‡ Ù„Ø§Ú¯ÛŒÙ† Ú©Ø±Ø¯Ù‡
                action="GENERATE_REPORT",
                details=f"Generated progress report for project ID {project_id}"
            )

            return report

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªÙˆÙ„ÛŒØ¯ Ú¯Ø²Ø§Ø±Ø´ Ù¾Ø±ÙˆÚ˜Ù‡ {project_id}: {e}")
            return report
        finally:
            session.close()

        # Ù…ØªØ¯Ù‡Ø§ÛŒ Ù…Ø¯ÛŒØ±ÛŒØª Ù¾Ø±ÙˆÚ˜Ù‡ Ùˆ Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ (Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ Ø§Ø² MIVRegistry)
        # --------------------------------------------------------------------

    def rename_project(self, project_id, new_name, user="system"):
        """Ù†Ø§Ù… ÛŒÚ© Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ø§ ØªØºÛŒÛŒØ± Ù…ÛŒâ€ŒØ¯Ù‡Ø¯."""
        session = self.get_session()
        try:
            project = session.query(Project).filter(Project.id == project_id).first()
            if project:
                original_name = project.name
                # Ø¨Ø±Ø±Ø³ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ù†Ø§Ù… Ø¬Ø¯ÛŒØ¯ ØªÚ©Ø±Ø§Ø±ÛŒ Ù†Ø¨Ø§Ø´Ø¯
                name_exists = session.query(Project.id).filter(Project.name == new_name).first()
                if name_exists:
                    return False, f"Ù¾Ø±ÙˆÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ù†Ø§Ù… '{new_name}' Ø§Ø² Ù‚Ø¨Ù„ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯."

                project.name = new_name
                session.commit()
                self.log_activity(user, "RENAME_PROJECT", f"Project '{original_name}' renamed to '{new_name}'")
                return True, f"Ù†Ø§Ù… Ù¾Ø±ÙˆÚ˜Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ '{new_name}' ØªØºÛŒÛŒØ± ÛŒØ§ÙØª."
            return False, "Ù¾Ø±ÙˆÚ˜Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯."
        except Exception as e:
            session.rollback()
            return False, f"Ø®Ø·Ø§ Ø¯Ø± ØªØºÛŒÛŒØ± Ù†Ø§Ù… Ù¾Ø±ÙˆÚ˜Ù‡: {e}"
        finally:
            session.close()

    def copy_line_to_project(self, line_no, from_project_id, to_project_id, user="system"):
        """ØªÙ…Ø§Ù… Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ MIV ÛŒÚ© Ø®Ø· Ø±Ø§ Ø§Ø² Ù¾Ø±ÙˆÚ˜Ù‡â€ŒØ§ÛŒ Ø¨Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ Ø¯ÛŒÚ¯Ø± Ú©Ù¾ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        session = self.get_session()
        try:
            records_to_copy = session.query(MIVRecord).filter(
                MIVRecord.project_id == from_project_id,
                MIVRecord.line_no == line_no
            ).all()

            if not records_to_copy:
                return False, "Ù‡ÛŒÚ† Ø±Ú©ÙˆØ±Ø¯ÛŒ Ø¨Ø±Ø§ÛŒ Ú©Ù¾ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."

            for record in records_to_copy:
                # Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÚ©Ø±Ø§Ø±ÛŒ Ø´Ø¯Ù† ØªÚ¯ØŒ ÛŒÚ© Ù¾Ø³ÙˆÙ†Ø¯ Ø¨Ù‡ Ø¢Ù† Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                new_tag = f"{record.miv_tag}-COPY-{datetime.now().strftime('%f')}"

                new_record = MIVRecord(
                    project_id=to_project_id,
                    line_no=record.line_no,
                    miv_tag=new_tag,
                    location=record.location,
                    status=record.status,
                    comment=f"Copied from project ID {from_project_id}",
                    registered_for=record.registered_for,
                    registered_by=user,
                    is_complete=record.is_complete,
                    last_updated=datetime.now()
                )
                session.add(new_record)

            session.commit()
            self.log_activity(user, "COPY_LINE",
                              f"Line '{line_no}' copied from project {from_project_id} to {to_project_id}")
            return True, "Ø®Ø· Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ú©Ù¾ÛŒ Ø´Ø¯."
        except Exception as e:
            session.rollback()
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ú©Ù¾ÛŒ Ú©Ø±Ø¯Ù† Ø®Ø·: {e}"
        finally:
            session.close()

    def check_duplicates_in_project(self, project_id, column_name='miv_tag'):
        """Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø§ Ù…Ù‚Ø¯Ø§Ø± ØªÚ©Ø±Ø§Ø±ÛŒ Ø¯Ø± ÛŒÚ© Ø³ØªÙˆÙ† Ø®Ø§Øµ (Ù…Ø«Ù„ miv_tag) Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        session = self.get_session()
        try:
            if not hasattr(MIVRecord, column_name):
                return None, f"Ø³ØªÙˆÙ†ÛŒ Ø¨Ø§ Ù†Ø§Ù… '{column_name}' Ø¯Ø± Ù…Ø¯Ù„ MIVRecord ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯."

            column = getattr(MIVRecord, column_name)

            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ù‚Ø§Ø¯ÛŒØ± ØªÚ©Ø±Ø§Ø±ÛŒ
            duplicates_query = session.query(column, func.count(MIVRecord.id).label('count')). \
                filter(MIVRecord.project_id == project_id). \
                group_by(column). \
                having(func.count(MIVRecord.id) > 1).subquery()

            # Ú¯Ø±ÙØªÙ† ØªÙ…Ø§Ù… Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø³ØªÙˆÙ†Ø´Ø§Ù† ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø³Øª
            final_query = session.query(MIVRecord).join(
                duplicates_query, column == duplicates_query.c[column_name]
            ).order_by(column)

            return final_query.all(), None
        except Exception as e:
            return None, f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù…ÙˆØ§Ø±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ: {e}"
        finally:
            session.close()

    def is_line_complete(self, project_id, line_no):
        session = self.get_session()
        try:
            mto_items = session.query(MTOItem).filter(
                MTOItem.project_id == project_id,
                MTOItem.line_no == line_no
            ).all()

            if not mto_items:
                return False

            for item in mto_items:
                if item.item_type and 'pipe' in item.item_type.lower():
                    total_required = item.length_m or 0
                else:
                    total_required = item.quantity or 0

                total_used = session.query(func.coalesce(func.sum(MTOConsumption.used_qty), 0.0)) \
                    .filter(MTOConsumption.mto_item_id == item.id).scalar()

                if total_used < total_required:
                    return False
            return True
        finally:
            session.close()

    def get_enriched_line_progress(self, project_id, line_no, readonly=True):
        """
        Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØª Ù…ØªØ±ÛŒØ§Ù„ ÛŒÚ© Ø®Ø· Ø±Ø§ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ØªÚ©Ù…ÛŒÙ„ÛŒ Ø§Ø² MTOItem Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
        """
        session = self.get_session()
        try:
            # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ø±Ú©ÙˆØ±Ø¯ÛŒ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ Ù¾ÛŒØ´Ø±ÙØª Ù†Ø¨ÙˆØ¯ØŒ Ø¢Ù† Ø±Ø§ Ø§Ø² MTOItem Ø¨Ø³Ø§Ø²
            if not readonly:
                self.initialize_mto_progress_for_line(project_id, line_no)

            # Ø¬ÙˆÛŒÙ† MTOProgress Ø¨Ø§ MTOItem Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¨ÛŒØ´ØªØ±
            results = session.query(
                MTOProgress,
                MTOItem.p1_bore_in,
                MTOItem.item_type
            ).join(
                MTOItem, MTOProgress.mto_item_id == MTOItem.id
            ).filter(
                MTOProgress.project_id == project_id,
                MTOProgress.line_no == line_no
            ).all()

            progress_data = []
            for item in results:
                progress_record, p1_bore, item_type = item
                progress_data.append({
                    "mto_item_id": progress_record.mto_item_id,
                    "Item Code": progress_record.item_code,
                    "Description": progress_record.description,
                    "Unit": progress_record.unit,
                    "Total Qty": progress_record.total_qty or 0,
                    "Used Qty": progress_record.used_qty or 0,
                    "Remaining Qty": progress_record.remaining_qty or 0,
                    "Bore": p1_bore,
                    "Type": item_type
                })
            return progress_data
        except Exception as e:
            logging.error(f"Error in get_enriched_line_progress for line {line_no}: {e}")
            return []
        finally:
            session.close()

    def initialize_mto_progress_for_line(self, project_id, line_no):
        session = self.get_session()
        try:
            mto_items = session.query(MTOItem).filter(
                MTOItem.project_id == project_id,
                MTOItem.line_no == line_no
            ).all()

            for item in mto_items:
                exists = session.query(MTOProgress).filter(MTOProgress.mto_item_id == item.id).first()
                if not exists:
                    # --- CHANGE: Ø­Ø°Ù ØªØ¨Ø¯ÛŒÙ„ ÙˆØ§Ø­Ø¯ ---
                    is_pipe = item.item_type and 'pipe' in item.item_type.lower()
                    if is_pipe:
                        total_required = item.length_m or 0 # Ø¯ÛŒÚ¯Ø± Ø¶Ø±Ø¨ Ø¯Ø± Û±Û°Û°Û° Ù†Ø¯Ø§Ø±ÛŒÙ…
                    else:
                        total_required = item.quantity or 0

                    total_used = session.query(func.coalesce(func.sum(MTOConsumption.used_qty), 0.0)) \
                        .filter(MTOConsumption.mto_item_id == item.id).scalar()

                    new_progress = MTOProgress(
                        project_id=project_id,
                        line_no=line_no,
                        mto_item_id=item.id,
                        item_code=item.item_code,
                        description=item.description,
                        unit=item.unit,
                        total_qty=round(total_required, 2),
                        used_qty=round(total_used, 2),
                        remaining_qty=round(max(0, total_required - total_used), 2),
                        last_updated=datetime.now()
                    )
                    session.add(new_progress)
            session.commit()
        except Exception as e:
            session.rollback()
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª Ù¾ÛŒØ´â€ŒÙØ±Ø¶ MTO Progress: {e}")
        finally:
            session.close()

    def get_data_as_dataframe(self, model_class, project_id=None):
        """
        Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÛŒÚ© Ø¬Ø¯ÙˆÙ„ (Ù…Ø¯Ù„) Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª Ù¾Ø§Ù†Ø¯Ø§Ø² DataFrame Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
        Ø§ÛŒÙ† Ù…ØªØ¯ Ø¨Ø±Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ú¯Ø±ÙØªÙ† Ø§Ú©Ø³Ù„ Ø¨Ø³ÛŒØ§Ø± Ù…ÙÛŒØ¯ Ø§Ø³Øª.
        """
        session = self.get_session()
        try:
            query = session.query(model_class)
            if project_id and hasattr(model_class, 'project_id'):
                query = query.filter(model_class.project_id == project_id)

            # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² pd.read_sql Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø§ÛŒÛŒ Ø¨Ù‡ØªØ±
            df = pd.read_sql(query.statement, session.bind)
            return df
        except Exception as e:
            print(f"Error converting table to DataFrame: {e}")
            return pd.DataFrame()
        finally:
            session.close()

    def backup_database(self, backup_dir="."):
        """Ø§Ø² Ú©Ù„ ÙØ§ÛŒÙ„ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ ÛŒÚ© Ù†Ø³Ø®Ù‡ Ù¾Ø´ØªÛŒØ¨Ø§Ù† ØªÙ‡ÛŒÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        import shutil

        db_file = self.engine.url.database
        if not os.path.exists(db_file):
            return False, "ÙØ§ÛŒÙ„ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯."

        try:
            if not os.path.exists(backup_dir):
                os.makedirs(backup_dir)

            backup_name = f"backup_{os.path.basename(db_file)}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.db"
            backup_path = os.path.join(backup_dir, backup_name)

            shutil.copy2(db_file, backup_path)
            return True, f"Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± Ù…Ø³ÛŒØ± {backup_path} Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯."
        except Exception as e:
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø´ØªÛŒØ¨Ø§Ù†â€ŒÚ¯ÛŒØ±ÛŒ: {e}"

    def update_mto_progress(self, project_id, line_no, updates):  # ENHANCE: Add logging for debugging
        """
        Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¬Ø¯ÙˆÙ„ mto_progress Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…ØµØ±ÙÛŒ Ø¬Ø¯ÛŒØ¯.
        updates: Ù„ÛŒØ³ØªÛŒ Ø§Ø² ØªØ§Ù¾Ù„â€ŒÙ‡Ø§ (item_code, qty, unit, description)
        """
        session = self.get_session()
        try:
            for item_code, qty, unit, desc in updates:
                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ Ø§Ø² MTOItem
                query = session.query(MTOItem).filter(
                    MTOItem.project_id == project_id,
                    MTOItem.line_no == line_no
                )

                if item_code and str(item_code).strip():
                    query = query.filter(MTOItem.item_code == str(item_code).strip())
                else:
                    query = query.filter(MTOItem.description == str(desc).strip())

                mto_items = query.all()

                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Total Qty
                total_qty = 0
                for mto_item in mto_items:
                    if mto_item.item_type and "pipe" in mto_item.item_type.lower():
                        total_qty += mto_item.length_m or 0
                    else:
                        total_qty += mto_item.quantity or 0

                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Used Qty
                used_qty = 0
                for mto_item in mto_items:
                    used_qty += (
                                    session.query(func.coalesce(func.sum(MTOConsumption.used_qty), 0.0))
                                    .filter(MTOConsumption.mto_item_id == mto_item.id)
                                    .scalar()
                                ) or 0

                remaining_qty = max(0, total_qty - used_qty)

                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ÛŒØ§ Ø³Ø§Ø®Øª Ø±Ú©ÙˆØ±Ø¯ Ø¯Ø± MTOProgress
                progress_row = session.query(MTOProgress).filter(
                    MTOProgress.line_no == line_no,
                    MTOProgress.item_code == (item_code or "")
                ).first()

                if progress_row:
                    progress_row.total_qty = total_qty
                    progress_row.used_qty = used_qty
                    progress_row.remaining_qty = remaining_qty
                    progress_row.last_updated = datetime.now()
                else:
                    new_progress = MTOProgress(
                        line_no=line_no,
                        item_code=item_code or "",
                        description=desc,
                        unit=unit,
                        total_qty=total_qty,
                        used_qty=used_qty,
                        remaining_qty=remaining_qty,
                        last_updated=datetime.now()
                    )
                    session.add(new_progress)

            session.commit()
        except Exception as e:
            session.rollback()
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ mto_progress: {e}")
        finally:
            session.close()

    def get_used_qty(self, project_id, line_no, item_code=None, description=None):
        session = self.get_session()
        try:
            query = session.query(func.coalesce(func.sum(MTOConsumption.used_qty), 0.0)) \
                .join(MTOItem, MTOConsumption.mto_item_id == MTOItem.id) \
                .filter(MTOItem.project_id == project_id, MTOItem.line_no == line_no)

            if item_code and str(item_code).strip():
                query = query.filter(MTOItem.item_code == str(item_code).strip())
            elif description:
                query = query.filter(MTOItem.description == str(description).strip())

            return query.scalar() or 0
        finally:
            session.close()

    def suggest_line_no(self, project_id, line_no_input):
        session = self.get_session()
        try:
            all_lines = [x[0] for x in session.query(MTOItem.line_no)
            .filter(MTOItem.project_id == project_id)
            .distinct().all()]
            norm_input = str(line_no_input).replace(" ", "").lower()
            normalized_lines = {line: str(line).replace(" ", "").lower() for line in all_lines}

            best_match = None
            best_ratio = 0
            for original, normalized in normalized_lines.items():
                ratio = difflib.SequenceMatcher(None, norm_input, normalized).ratio()
                if ratio > best_ratio:
                    best_ratio = ratio
                    best_match = original

            return best_match if best_ratio > 0.6 else None
        finally:
            session.close()

    # --------------------------------------------------------------------
    # Ù…ØªØ¯Ù‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± API Ø¬Ø¯ÛŒØ¯ ØµØ¯Ø§ Ø²Ø¯ÛŒÙ…
    # --------------------------------------------------------------------

    def get_lines_for_project(self, project_id):
        """ØªÙ…Ø§Ù… Ø´Ù…Ø§Ø±Ù‡ Ø®Ø·â€ŒÙ‡Ø§ÛŒ Ù…ØªÙ…Ø§ÛŒØ² Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
        session = self.get_session()
        try:
            # Ø§Ø² Ø¬Ø¯ÙˆÙ„ MTOItem Ø´Ù…Ø§Ø±Ù‡ Ø®Ø·ÙˆØ· Ø±Ø§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ…
            lines = session.query(MTOItem.line_no).filter(MTOItem.project_id == project_id).distinct().order_by(
                MTOItem.line_no).all()
            # Ù†ØªÛŒØ¬Ù‡ Ú©ÙˆØ¦Ø±ÛŒ Ù„ÛŒØ³ØªÛŒ Ø§Ø² tupleÙ‡Ø§Ø³ØªØŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡ Ù„ÛŒØ³Øª Ø±Ø´ØªÙ‡ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            return [line[0] for line in lines]
        except Exception as e:
            logging.error(f"Error fetching lines for project {project_id}: {e}")
            return []
        finally:
            session.close()

    def get_activity_logs(self, limit=100, action_filter=None):
        """Ø¢Ø®Ø±ÛŒÙ† N Ø±Ú©ÙˆØ±Ø¯ Ø§Ø² Ø¬Ø¯ÙˆÙ„ Ù„Ø§Ú¯ ÙØ¹Ø§Ù„ÛŒØªâ€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
        session = self.get_session()
        try:
            query = session.query(ActivityLog)
            if action_filter:
                query = query.filter(ActivityLog.action == action_filter)

            return query.order_by(ActivityLog.timestamp.desc()).limit(limit).all()
        except Exception as e:
            logging.error(f"Error fetching activity logs: {e}")
            return []
        finally:
            session.close()

    def get_project_analytics(self, project_id):
        """Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ Ùˆ Ø¢Ù…Ø§Ø±ÛŒ ÛŒÚ© Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ø´Ø¨ÙˆØ±Ø¯ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        session = self.get_session()
        try:
            # 1. ØªØ­Ù„ÛŒÙ„ ÙØ¹Ø§Ù„ÛŒØª Ú©Ø§Ø±Ø¨Ø±Ø§Ù† (ØªØ¹Ø¯Ø§Ø¯ MIV Ø«Ø¨Øª Ø´Ø¯Ù‡ ØªÙˆØ³Ø· Ù‡Ø± Ú©Ø§Ø±Ø¨Ø±)
            user_activity = session.query(
                MIVRecord.registered_by,
                func.count(MIVRecord.id).label('miv_count')
            ).filter(MIVRecord.project_id == project_id).group_by(MIVRecord.registered_by).order_by(
                func.count(MIVRecord.id).desc()).all()

            # 2. ØªØ­Ù„ÛŒÙ„ Ù…ØµØ±Ù Ù…ØªØ±ÛŒØ§Ù„ (Ù¾Ø± Ù…ØµØ±Ùâ€ŒØªØ±ÛŒÙ† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§)
            material_consumption = session.query(
                MTOItem.description,
                func.sum(MTOConsumption.used_qty).label('total_used')
            ).join(MTOConsumption, MTOItem.id == MTOConsumption.mto_item_id) \
                .filter(MTOItem.project_id == project_id) \
                .group_by(MTOItem.description).order_by(func.sum(MTOConsumption.used_qty).desc()).limit(10).all()

            # 3. ØªØ­Ù„ÛŒÙ„ ÙˆØ¶Ø¹ÛŒØª MIV Ù‡Ø§
            status_distribution = session.query(
                MIVRecord.status,
                func.count(MIVRecord.id).label('status_count')
            ).filter(MIVRecord.project_id == project_id, MIVRecord.status != None) \
                .group_by(MIVRecord.status).all()

            return {
                "user_activity": [{"user": user, "count": count} for user, count in user_activity],
                "material_consumption": [{"material": desc, "total_used": used} for desc, used in material_consumption],
                "status_distribution": [{"status": status, "count": count} for status, count in status_distribution]
            }
        except Exception as e:
            logging.error(f"Error fetching project analytics for project {project_id}: {e}")
            return {}
        finally:
            session.close()

    # --------------------------------------------------------------------
    # Ù…ØªØ¯Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ú¯Ø°Ø§Ø±Ø´ Ú¯ÛŒØ±ÛŒ
    # --------------------------------------------------------------------

    def get_project_mto_summary(self, project_id: int, **filters) -> Dict[str, Any]:
        """
        --- CHANGE: Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÙØ²ÙˆØ¯Ù† ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØªÙ‡ Ùˆ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ ---
        Ú¯Ø²Ø§Ø±Ø´ Ø®Ù„Ø§ØµÙ‡ Ù¾ÛŒØ´Ø±ÙØª Ù…ØªØ±ÛŒØ§Ù„ (MTO Summary) Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ù¾Ø±ÙˆÚ˜Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        session = self.get_session()
        try:
            # Ú©ÙˆØ¦Ø±ÛŒ Ù¾Ø§ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù…Ø¹â€ŒØ¨Ù†Ø¯ÛŒ Ù¾ÛŒØ´Ø±ÙØª Ø¯Ø± Ø³Ø·Ø­ Ù¾Ø±ÙˆÚ˜Ù‡
            summary_query = session.query(
                MTOProgress.item_code,
                MTOProgress.description,
                MTOProgress.unit,
                func.sum(MTOProgress.total_qty).label("total_required"),
                func.sum(MTOProgress.used_qty).label("total_used")
            ).filter(MTOProgress.project_id == project_id).group_by(
                MTOProgress.item_code, MTOProgress.description, MTOProgress.unit
            )

            # --- Filters ---
            if filters.get('item_code'):
                summary_query = summary_query.filter(MTOProgress.item_code.ilike(f"%{filters['item_code']}%"))
            if filters.get('description'):
                summary_query = summary_query.filter(MTOProgress.description.ilike(f"%{filters['description']}%"))

            # Ø§Ø¬Ø±Ø§ÛŒ Ú©ÙˆØ¦Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† ØªÙ…Ø§Ù… Ù†ØªØ§ÛŒØ¬ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡
            all_results = summary_query.all()

            report_data = []
            total_required_sum = 0
            total_used_sum = 0

            for row in all_results:
                total_required_sum += row.total_required
                total_used_sum += row.total_used
                remaining = row.total_required - row.total_used
                progress = (row.total_used / row.total_required * 100) if row.total_required > 0 else 0

                # ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø¨Ø¹Ø¯ Ø§Ø² Ù…Ø­Ø§Ø³Ø¨Ù‡ (Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ´Ø±ÙØª)
                min_progress = filters.get('min_progress')
                max_progress = filters.get('max_progress')
                if (min_progress is not None and progress < min_progress) or \
                        (max_progress is not None and progress > max_progress):
                    continue

                report_data.append({
                    "Item Code": row.item_code or "N/A",
                    "Description": row.description,
                    "Unit": row.unit,
                    "Total Required": round(row.total_required, 2),
                    "Total Used": round(row.total_used, 2),
                    "Remaining": round(remaining, 2),
                    "Progress (%)": round(progress, 2)
                })

            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
            sort_by = filters.get('sort_by', 'Item Code')
            sort_order = filters.get('sort_order', 'asc')
            reverse = sort_order == 'desc'
            if sort_by in report_data[0]:
                report_data.sort(key=lambda x: x[sort_by], reverse=reverse)

            # Ø³Ø§Ø®Øª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø®Ø±ÙˆØ¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ
            output = {
                "summary": {
                    "total_unique_items": len(report_data),
                    "grand_total_required": round(total_required_sum, 2),
                    "grand_total_used": round(total_used_sum, 2),
                    "overall_progress": round(
                        (total_used_sum / total_required_sum * 100) if total_required_sum > 0 else 0, 2)
                },
                "data": report_data
            }
            return output

        except Exception as e:
            logging.error(f"Error in get_project_mto_summary: {e}")
            return {"summary": {}, "data": []}
        finally:
            session.close()

    def get_project_line_status_list(self, project_id: int) -> List[Dict[str, Any]]:
        """
        Ú¯Ø²Ø§Ø±Ø´ Ù„ÛŒØ³Øª ÙˆØ¶Ø¹ÛŒØª Ø®Ø·ÙˆØ· (Line Status List) Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù¾Ø±ÙˆÚ˜Ù‡ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        session = self.get_session()
        try:
            lines = self.get_lines_for_project(project_id)
            report_data = []
            for line_no in lines:
                progress_info = self.get_line_progress(project_id, line_no)
                last_activity = session.query(func.max(MIVRecord.last_updated)).filter(
                    MIVRecord.project_id == project_id,
                    MIVRecord.line_no == line_no
                ).scalar()

                status = "Complete" if progress_info.get("percentage", 0) >= 99.99 else "In-Progress"

                report_data.append({
                    "Line No": line_no,
                    "Progress (%)": progress_info.get("percentage", 0),
                    "Status": status,
                    "Last Activity Date": last_activity.strftime('%Y-%m-%d') if last_activity else "N/A"
                })
            return sorted(report_data, key=lambda x: x['Line No'])
        except Exception as e:
            logging.error(f"Error in get_project_line_status_list: {e}")
            return []
        finally:
            session.close()

    def get_detailed_line_report(self, project_id: int, line_no: str) -> Dict[str, List]:
        """
        Ú¯Ø²Ø§Ø±Ø´ Ú©Ø§Ù…Ù„ Ùˆ Ø¯Ùˆ Ø¨Ø®Ø´ÛŒ ÛŒÚ© Ø®Ø· Ø®Ø§Øµ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        session = self.get_session()
        try:
            # Ø¨Ø®Ø´ Ø§ÙˆÙ„: Ù„ÛŒØ³Øª Ù…ØªØ±ÛŒØ§Ù„ (Bill of Materials)
            bom = self.get_enriched_line_progress(project_id, line_no,
                                                  readonly=False)  # Readonly=False to ensure it's initialized

            # Ø¨Ø®Ø´ Ø¯ÙˆÙ…: ØªØ§Ø±ÛŒØ®Ú†Ù‡ MIV Ù‡Ø§
            miv_history_query = session.query(MIVRecord).filter(
                MIVRecord.project_id == project_id,
                MIVRecord.line_no == line_no
            ).order_by(desc(MIVRecord.last_updated)).all()

            miv_history = [
                {
                    "MIV Tag": r.miv_tag,
                    "Registered By": r.registered_by,
                    "Date": r.last_updated.strftime('%Y-%m-%d %H:%M'),
                    "Status": r.status,
                    "Comment": r.comment
                } for r in miv_history_query
            ]

            return {
                "bill_of_materials": bom,
                "miv_history": miv_history
            }
        except Exception as e:
            logging.error(f"Error in get_detailed_line_report: {e}")
            return {"bill_of_materials": [], "miv_history": []}
        finally:
            session.close()

    def get_shortage_report(self, project_id: int, line_no: str = None) -> List[Dict[str, Any]]:
        """
        Ú¯Ø²Ø§Ø±Ø´ Ú©Ø³Ø±ÛŒ Ù…ØªØ±ÛŒØ§Ù„ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ù¾Ø±ÙˆÚ˜Ù‡ ÛŒØ§ ÛŒÚ© Ø®Ø· Ø®Ø§Øµ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        ÙÙ‚Ø· Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ú©Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ (Remaining) Ø¨ÛŒØ´ØªØ± Ø§Ø² ØµÙØ± Ø¨Ø§Ø´Ø¯.
        """
        session = self.get_session()
        try:
            query = session.query(
                MTOProgress.item_code,
                MTOProgress.description,
                MTOProgress.unit,
                func.sum(MTOProgress.total_qty).label("total_required"),
                func.sum(MTOProgress.used_qty).label("total_used")
            ).filter(MTOProgress.project_id == project_id)

            # Ø§Ú¯Ø± Ø´Ù…Ø§Ø±Ù‡ Ø®Ø· Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯ØŒ Ú©ÙˆØ¦Ø±ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¢Ù† Ø®Ø· Ù…Ø­Ø¯ÙˆØ¯ Ú©Ù†
            if line_no:
                query = query.filter(MTOProgress.line_no == line_no)

            # ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ú©Ø³Ø±ÛŒ Ø¯Ø§Ø±Ù†Ø¯
            results = query.group_by(
                MTOProgress.item_code, MTOProgress.description, MTOProgress.unit
            ).having(
                func.sum(MTOProgress.total_qty) > func.sum(MTOProgress.used_qty)
            ).order_by(MTOProgress.item_code).all()

            report_data = []
            for row in results:
                remaining = row.total_required - row.total_used
                progress = (row.total_used / row.total_required * 100) if row.total_required > 0 else 0
                report_data.append({
                    "Item Code": row.item_code or "N/A",
                    "Description": row.description,
                    "Unit": row.unit,
                    "Total Required": round(row.total_required, 2),
                    "Total Used": round(row.total_used, 2),
                    "Remaining": round(remaining, 2),
                    "Progress (%)": round(progress, 2)
                })
            return report_data
        except Exception as e:
            logging.error(f"Error in get_shortage_report: {e}")
            return []
        finally:
            session.close()

    def get_spool_inventory_report(self, **filters) -> Dict[str, Any]:
        """
        --- CHANGE: Ø¨Ø§Ø²Ù†ÙˆÛŒØ³ÛŒ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Ø§ÙØ²ÙˆØ¯Ù† ÙÛŒÙ„ØªØ±ØŒ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØµÙØ­Ù‡â€ŒØ¨Ù†Ø¯ÛŒ ---
        Ú¯Ø²Ø§Ø±Ø´ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§Ù†Ø¨Ø§Ø± Ø§Ø³Ù¾ÙˆÙ„ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        session = self.get_session()
        try:
            query = session.query(Spool, SpoolItem).join(
                SpoolItem, Spool.id == SpoolItem.spool_id_fk
            ).filter(
                (SpoolItem.qty_available > 0.001) | (SpoolItem.length > 0.001)
            )

            # --- Filters ---
            if filters.get('spool_id'):
                query = query.filter(Spool.spool_id.ilike(f"%{filters['spool_id']}%"))
            if filters.get('location'):
                query = query.filter(Spool.location.ilike(f"%{filters['location']}%"))
            if filters.get('component_type'):
                query = query.filter(SpoolItem.component_type.ilike(f"%{filters['component_type']}%"))
            if filters.get('material'):
                query = query.filter(SpoolItem.material.ilike(f"%{filters['material']}%"))

            # --- Sorting ---
            sort_by = filters.get('sort_by', 'spool_id')
            sort_order = filters.get('sort_order', 'asc')
            sort_column = getattr(Spool, sort_by, getattr(SpoolItem, sort_by, Spool.spool_id))
            query = query.order_by(desc(sort_column) if sort_order == 'desc' else sort_column)

            # --- Pagination ---
            page = filters.get('page', 1)
            per_page = filters.get('per_page', 20)
            total_records = query.count()
            total_pages = (total_records + per_page - 1) // per_page

            results = query.offset((page - 1) * per_page).limit(per_page).all()

            report_data = []
            for spool, item in results:
                is_pipe = "PIPE" in (item.component_type or "").upper()
                report_data.append({
                    "Spool ID": spool.spool_id,
                    "Location": spool.location,
                    "Component Type": item.component_type,
                    "Item Code": item.item_code,
                    "Material": item.material,
                    "Bore1": item.p1_bore,
                    "Schedule": item.schedule,
                    "Available": round(item.length if is_pipe else item.qty_available, 2),
                    "Unit": "m" if is_pipe else "pcs"
                })

            return {
                "pagination": {
                    "total_records": total_records,
                    "total_pages": total_pages,
                    "current_page": page,
                    "per_page": per_page
                },
                "data": report_data
            }
        except Exception as e:
            logging.error(f"Error in get_spool_inventory_report: {e}")
            return {"pagination": {}, "data": []}
        finally:
            session.close()

    def get_spool_consumption_history(self) -> List[Dict[str, Any]]:
        """
        Ú¯Ø²Ø§Ø±Ø´ ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…ØµØ±Ù Ø§Ø³Ù¾ÙˆÙ„â€ŒÙ‡Ø§ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        session = self.get_session()
        try:
            history_query = session.query(
                SpoolConsumption.timestamp,
                Spool.spool_id,
                SpoolItem.component_type,
                SpoolConsumption.used_qty,
                MIVRecord.miv_tag,
                MIVRecord.line_no
            ).join(
                SpoolItem, SpoolConsumption.spool_item_id == SpoolItem.id
            ).join(
                Spool, SpoolConsumption.spool_id == Spool.id
            ).join(
                MIVRecord, SpoolConsumption.miv_record_id == MIVRecord.id
            ).order_by(desc(SpoolConsumption.timestamp)).all()

            report_data = []
            for row in history_query:
                is_pipe = "PIPE" in (row.component_type or "").upper()
                unit = "m" if is_pipe else "pcs"
                report_data.append({
                    "Timestamp": row.timestamp.strftime('%Y-%m-%d %H:%M'),
                    "Spool ID": row.spool_id,
                    "Component Type": row.component_type,
                    "Used Qty": f"{row.used_qty:.2f} {unit}",
                    "Consumed in MIV": row.miv_tag,
                    "For Line No": row.line_no
                })
            return report_data
        except Exception as e:
            logging.error(f"Error in get_spool_consumption_history: {e}")
            return []
        finally:
            session.close()

    def get_report_analytics(self, project_id: int, report_name: str, **params) -> Dict[str, Any]:
        """
        --- NEW: Ù…ØªØ¯ Ø¬Ø¯ÛŒØ¯ Ùˆ Ù‚Ø¯Ø±ØªÙ…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ ØªÙˆÙ„ÛŒØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ Ùˆ Ø¢Ù…Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±Ù‡Ø§ ---
        """
        session = self.get_session()
        try:
            # Ú¯Ø²Ø§Ø±Ø´ Ø§ÙˆÙ„: ØªÙˆØ²ÛŒØ¹ Ù¾ÛŒØ´Ø±ÙØª Ø®Ø·ÙˆØ· (Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ù…ÛŒÙ„Ù‡â€ŒØ§ÛŒ ÛŒØ§ Ø¯Ø§ÛŒØ±Ù‡â€ŒØ§ÛŒ)
            if report_name == 'line_progress_distribution':
                lines = self.get_project_line_status_list(project_id)
                bins = {"0-25%": 0, "25-50%": 0, "50-75%": 0, "75-99%": 0, "100%": 0}
                for line in lines:
                    p = line['Progress (%)']
                    if p < 25:
                        bins["0-25%"] += 1
                    elif p < 50:
                        bins["25-50%"] += 1
                    elif p < 75:
                        bins["50-75%"] += 1
                    elif p < 100:
                        bins["75-99%"] += 1
                    else:
                        bins["100%"] += 1

                return {
                    "title": "ØªÙˆØ²ÛŒØ¹ Ù¾ÛŒØ´Ø±ÙØª Ø®Ø·ÙˆØ·",
                    "type": "bar",
                    "data": {
                        "labels": list(bins.keys()),
                        "datasets": [{"label": "ØªØ¹Ø¯Ø§Ø¯ Ø®Ø·ÙˆØ·", "data": list(bins.values())}]
                    }
                }

            # Ú¯Ø²Ø§Ø±Ø´ Ø¯ÙˆÙ…: Ù…ØµØ±Ù Ù…ØªØ±ÛŒØ§Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹ (Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø§ÛŒØ±Ù‡â€ŒØ§ÛŒ)
            elif report_name == 'material_usage_by_type':
                query = session.query(
                    MTOItem.item_type,
                    func.sum(MTOProgress.used_qty).label('total_used')
                ).join(MTOProgress, MTOItem.id == MTOProgress.mto_item_id) \
                    .filter(MTOProgress.project_id == project_id, MTOItem.item_type != None) \
                    .group_by(MTOItem.item_type).order_by(desc('total_used')).limit(10)

                results = query.all()
                return {
                    "title": "Û±Û° Ù†ÙˆØ¹ Ù…ØªØ±ÛŒØ§Ù„ Ù¾Ø± Ù…ØµØ±Ù",
                    "type": "pie",
                    "data": {
                        "labels": [r.item_type for r in results],
                        "datasets": [{"data": [round(r.total_used, 2) for r in results]}]
                    }
                }

            # Ú¯Ø²Ø§Ø±Ø´ Ø³ÙˆÙ…: ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…ØµØ±Ù Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù† (Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ø®Ø·ÛŒ)
            elif report_name == 'consumption_over_time':
                # Ø§ÛŒÙ† Ú¯Ø²Ø§Ø±Ø´ Ø³Ø±Ø§Ø³Ø±ÛŒ Ø§Ø³Øª Ùˆ Ø¨Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ ÙˆØ§Ø¨Ø³ØªÙ‡ Ù†ÛŒØ³Øª
                query = session.query(
                    func.strftime('%Y-%m-%d', SpoolConsumption.timestamp).label('date'),
                    func.count(SpoolConsumption.id).label('consumption_count')
                ).group_by('date').order_by('date')

                results = query.all()
                return {
                    "title": "ØªØ¹Ø¯Ø§Ø¯ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…ØµØ±ÙÛŒ Ø§Ø² Ø§Ù†Ø¨Ø§Ø± Ø§Ø³Ù¾ÙˆÙ„ Ø¯Ø± Ø·ÙˆÙ„ Ø²Ù…Ø§Ù†",
                    "type": "line",
                    "data": {
                        "labels": [r.date for r in results],
                        "datasets": [{"label": "ØªØ¹Ø¯Ø§Ø¯ Ù…ØµØ±Ù", "data": [r.consumption_count for r in results]}]
                    }
                }

            return {"error": "Report name not found"}, 404

        except Exception as e:
            logging.error(f"Error in get_report_analytics: {e}")
            return {"error": str(e)}, 500
        finally:
            session.close()

    # --------------------------------------------------------------------
    # Ù…ØªØ¯Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù¾ÙˆÙ„ MIV SPOOL
    # --------------------------------------------------------------------

    def get_mapped_spool_items(self, mto_item_type, p1_bore):
        """
        Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾ÙˆÙ„ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ú¯Ø§Ø´Øª Ø¨Ø±Ø§ÛŒ Ù†ÙˆØ¹ Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª Ùˆ Ø³Ø§ÛŒØ² (Bore) ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        ØªØ·Ø¨ÛŒÙ‚ Ø¨Ù‡â€ŒØµÙˆØ±Øª Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ± (startswith/contains) Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        """
        session = self.get_session()
        try:
            if not mto_item_type:
                return []

            mto_type_upper = str(mto_item_type).upper().strip()
            spool_equivalents = [mto_type_upper]  # Ù‡Ù…ÛŒØ´Ù‡ Ø®ÙˆØ¯ Ø¢ÛŒØªÙ… Ø±Ø§ Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† ÛŒÚ© Ø§Ø­ØªÙ…Ø§Ù„ Ø¯Ø± Ù†Ø¸Ø± Ø¨Ú¯ÛŒØ±

            # ğŸ”¹ Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ù†Ú¯Ø§Ø´Øª Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø¹Ø§Ø¯Ù„â€ŒÙ‡Ø§
            for key, aliases in SPOOL_TYPE_MAPPING.items():
                # Ø§Ú¯Ø± Ù†ÙˆØ¹ MTO Ø¨Ø§ ÛŒÚ©ÛŒ Ø§Ø² Ú©Ù„ÛŒØ¯Ù‡Ø§ ÛŒØ§ Ù…Ø¹Ø§Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¢Ù† ÛŒÚ©ÛŒ Ø¨ÙˆØ¯
                if mto_type_upper == key or mto_type_upper in aliases:
                    spool_equivalents.extend([key] + list(aliases))
                    break # ÙˆÙ‚ØªÛŒ Ú¯Ø±ÙˆÙ‡ Ø¯Ø±Ø³Øª Ù¾ÛŒØ¯Ø§ Ø´Ø¯ØŒ Ø§Ø² Ø­Ù„Ù‚Ù‡ Ø®Ø§Ø±Ø¬ Ø´Ùˆ

            # Ø­Ø°Ù Ù…ÙˆØ§Ø±Ø¯ ØªÚ©Ø±Ø§Ø±ÛŒ Ø§Ø² Ù„ÛŒØ³Øª Ù…Ø¹Ø§Ø¯Ù„â€ŒÙ‡Ø§
            spool_equivalents = list(set(spool_equivalents))

            query = session.query(SpoolItem).options(
                joinedload(SpoolItem.spool)
            ).filter(
                # Ø´Ø±Ø·: Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¨Ø²Ø±Ú¯ØªØ± Ø§Ø² ØµÙØ± Ø¨Ø§Ø´Ø¯
                (SpoolItem.qty_available > 0.001) | (SpoolItem.length > 0.001),
                # Ø´Ø±Ø·: Ù†ÙˆØ¹ Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª ÛŒÚ©ÛŒ Ø§Ø² Ù…ÙˆØ§Ø±Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù„ÛŒØ³Øª Ù…Ø¹Ø§Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø§Ø´Ø¯
                func.upper(SpoolItem.component_type).in_(spool_equivalents)
            )

            # ÙÛŒÙ„ØªØ± Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ø§ÛŒØ² (Bore) Ø§Ú¯Ø± Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ Ø¨ÙˆØ¯
            if p1_bore is not None:
                query = query.filter(SpoolItem.p1_bore == p1_bore)

            return query.all()

        except Exception as e:
            logging.error(f"Error fetching mapped spool items: {e}")
            return []
        finally:
            session.close()

    def register_spool_consumption(self, miv_record_id, spool_consumptions, user="system"):
        """
        Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ù…ØµØ±Ùâ€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾ÙˆÙ„ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ÛŒÚ© MIV Ù…Ø´Ø®Øµ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø«Ø¨Øª Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        spool_consumptions: list of dicts -> [{'spool_item_id': id, 'used_qty': qty}]
        """
        session = self.get_session()
        try:
            miv_record = session.query(MIVRecord).get(miv_record_id)
            if not miv_record:
                return False, "Ø±Ú©ÙˆØ±Ø¯ MIV ÛŒØ§ÙØª Ù†Ø´Ø¯."

            spool_ids_used = set()

            for consumption in spool_consumptions:
                spool_item_id = consumption['spool_item_id']
                used_qty = consumption['used_qty']

                spool_item = session.get(SpoolItem, spool_item_id)
                if not spool_item:
                    raise Exception(f"Ø¢ÛŒØªÙ… Ø§Ø³Ù¾ÙˆÙ„ Ø¨Ø§ Ø´Ù†Ø§Ø³Ù‡ {spool_item_id} ÛŒØ§ÙØª Ù†Ø´Ø¯.")

                if spool_item.qty_available < used_qty:
                    raise Exception(f"Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø¢ÛŒØªÙ… {spool_item.id} Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª.")

                # Û±. Ú©Ø§Ù‡Ø´ Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§Ø² Ø¢ÛŒØªÙ… Ø§Ø³Ù¾ÙˆÙ„
                spool_item.qty_available -= used_qty

                # Û². Ø«Ø¨Øª Ø±Ú©ÙˆØ±Ø¯ Ù…ØµØ±Ù Ø¯Ø± Ø¬Ø¯ÙˆÙ„ SpoolConsumption
                new_consumption = SpoolConsumption(
                    spool_item_id=spool_item.id,
                    spool_id=spool_item.spool.id,
                    miv_record_id=miv_record_id,
                    used_qty=used_qty,
                    timestamp=datetime.now()
                )
                session.add(new_consumption)
                spool_ids_used.add(str(spool_item.spool.id))

            session.commit()

            self.log_activity(
                user=user,
                action="REGISTER_SPOOL_CONSUMPTION",
                details=f"Spool items consumed for MIV ID {miv_record_id} from Spools: {', '.join(spool_ids_used)}"
            )
            return True, "Ù…ØµØ±Ù Ø§Ø³Ù¾ÙˆÙ„ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø«Ø¨Øª Ø´Ø¯."
        except Exception as e:
            session.rollback()
            logging.error(f"Error in register_spool_consumption: {e}")
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø«Ø¨Øª Ù…ØµØ±Ù Ø§Ø³Ù¾ÙˆÙ„: {e}"
        finally:
            session.close()

    def get_spool_consumptions_for_miv(self, miv_record_id):
        """
        ØªÙ…Ø§Ù… Ù…ØµØ±Ùâ€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾ÙˆÙ„ Ø«Ø¨Øªâ€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ ÛŒÚ© MIV Ø®Ø§Øµ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
        """
        session = self.get_session()
        try:
            return session.query(SpoolConsumption).filter(
                SpoolConsumption.miv_record_id == miv_record_id
            ).options(joinedload(SpoolConsumption.spool_item)).all()
        finally:
            session.close()

    def _get_matching_mto_progress_for_spool(self, session, spool_item, project_id, line_no):
        """
        ÛŒÚ© ØªØ§Ø¨Ø¹ Ú©Ù…Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø±Ú©ÙˆØ±Ø¯ MTOProgress Ù…ØªÙ†Ø§Ø¸Ø± Ø¨Ø§ ÛŒÚ© Ø¢ÛŒØªÙ… Ø§Ø³Ù¾ÙˆÙ„.
        ØªØ·Ø¨ÛŒÙ‚ Ø¨Ø± Ø§Ø³Ø§Ø³ Type Ùˆ Bore Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.
        """
        mto_item_type = spool_item.component_type
        p1_bore = spool_item.p1_bore

        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø¹Ø§Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù†ÙˆØ¹ Ú©Ø§Ù…Ù¾ÙˆÙ†Ù†Øª
        mto_type_upper = str(mto_item_type).upper().strip()
        spool_equivalents = [mto_type_upper]
        for key, aliases in SPOOL_TYPE_MAPPING.items():
            if mto_type_upper == key or mto_type_upper in aliases:
                spool_equivalents.extend([key] + list(aliases))
                break
        spool_equivalents = list(set(spool_equivalents))

        # Ø¬Ø³ØªØ¬Ùˆ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† MTOItem Ù…ØªÙ†Ø§Ø¸Ø±
        mto_item_query = session.query(MTOItem).filter(
            MTOItem.project_id == project_id,
            MTOItem.line_no == line_no,
            func.upper(MTOItem.item_type).in_(spool_equivalents)
        )
        if p1_bore is not None:
            mto_item_query = mto_item_query.filter(MTOItem.p1_bore_in == p1_bore)

        mto_item = mto_item_query.first()

        if mto_item:
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø±Ú©ÙˆØ±Ø¯ MTOProgress Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ø¢Ù† MTOItem
            return session.query(MTOProgress).filter(MTOProgress.mto_item_id == mto_item.id).first()

        return None


    # --------------------------------------------------------------------
    # Ù…ØªØ¯Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ  Ù…Ø¯ÛŒØ±ÛŒØª Ø§Ø³Ù¾ÙˆÙ„ Ù‡Ø§
    # --------------------------------------------------------------------

    def create_spool(self, spool_data: dict, items_data: list[dict]) -> Tuple[bool, str]:
        """
        ÛŒÚ© Ø§Ø³Ù¾ÙˆÙ„ Ø¬Ø¯ÛŒØ¯ Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒØ´ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        - Ø¹Ù…Ù„Ú©Ø±Ø¯: ØµØ­ÛŒØ­. Ø§Ø¨ØªØ¯Ø§ ÙˆØ¬ÙˆØ¯ Ø§Ø³Ù¾ÙˆÙ„ ØªÚ©Ø±Ø§Ø±ÛŒ Ø±Ø§ Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø³Ù¾Ø³ Ø§Ø³Ù¾ÙˆÙ„ Ùˆ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒØ´ Ø±Ø§ Ù…ÛŒâ€ŒØ³Ø§Ø²Ø¯.
        """
        session = self.get_session()
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ù†Ø¨ÙˆØ¯Ù† Ø´Ù†Ø§Ø³Ù‡ Ø§Ø³Ù¾ÙˆÙ„
            existing_spool = session.query(Spool.id).filter(Spool.spool_id == spool_data["spool_id"]).first()
            if existing_spool:
                return False, f"Ø§Ø³Ù¾ÙˆÙ„ÛŒ Ø¨Ø§ Ø´Ù†Ø§Ø³Ù‡ '{spool_data['spool_id']}' Ø§Ø² Ù‚Ø¨Ù„ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯."

            new_spool = Spool(
                spool_id=spool_data["spool_id"],
                location=spool_data.get("location"),
                # ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø² Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ Ø§Ø¶Ø§ÙÙ‡ Ø´ÙˆÙ†Ø¯
            )
            session.add(new_spool)
            session.flush()  # Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† ID Ø§Ø³Ù¾ÙˆÙ„ Ø¬Ø¯ÛŒØ¯

            for item in items_data:
                new_item = SpoolItem(
                    spool_id_fk=new_spool.id,
                    component_type=item.get("component_type"),
                    class_angle=item.get("class_angle"),
                    p1_bore=item.get("p1_bore"),
                    p2_bore=item.get("p2_bore"),
                    material=item.get("material"),
                    schedule=item.get("schedule"),
                    length=item.get("length"),
                    qty_available=item.get("qty_available"),
                    item_code=item.get("item_code")
                )
                session.add(new_item)

            session.commit()
            return True, f"Ø§Ø³Ù¾ÙˆÙ„ '{new_spool.spool_id}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯."
        except Exception as e:
            session.rollback()
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª Ø§Ø³Ù¾ÙˆÙ„: {e}")
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª Ø§Ø³Ù¾ÙˆÙ„: {e}"
        finally:
            session.close()

    def update_spool(self, spool_id: str, updated_data: dict, items_data: list[dict]) -> Tuple[bool, str]:
        """
        Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÛŒÚ© Ø§Ø³Ù¾ÙˆÙ„ Ùˆ ØªÙ…Ø§Ù… Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¢Ù† Ø±Ø§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        - Ø¹Ù…Ù„Ú©Ø±Ø¯: ØµØ­ÛŒØ­. Ø§Ø³Ù¾ÙˆÙ„ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù‡ØŒ ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒØ´ Ø±Ø§ Ø¢Ù¾Ø¯ÛŒØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ Ø±Ø§ Ú©Ø§Ù…Ù„Ø§Ù‹ Ù¾Ø§Ú© Ú©Ø±Ø¯Ù‡ Ùˆ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø±Ø§ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        session = self.get_session()
        try:
            spool = session.query(Spool).filter(Spool.spool_id == spool_id).first()
            if not spool:
                return False, "Ø§Ø³Ù¾ÙˆÙ„ ÛŒØ§ÙØª Ù†Ø´Ø¯."

            # Ø¢Ù¾Ø¯ÛŒØª ÙÛŒÙ„Ø¯Ù‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø§Ø³Ù¾ÙˆÙ„ (Ù…Ø§Ù†Ù†Ø¯ Ù„ÙˆÚ©ÛŒØ´Ù†)
            for key, value in updated_data.items():
                if hasattr(spool, key):
                    setattr(spool, key, value)

            # Ù¾Ø§Ú© Ú©Ø±Ø¯Ù† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ
            session.query(SpoolItem).filter(SpoolItem.spool_id_fk == spool.id).delete()
            session.flush()

            # Ø§ÙØ²ÙˆØ¯Ù† Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            for item in items_data:
                new_item = SpoolItem(
                    spool_id_fk=spool.id,
                    component_type=item.get("component_type"),
                    class_angle=item.get("class_angle"),
                    p1_bore=item.get("p1_bore"),
                    p2_bore=item.get("p2_bore"),
                    material=item.get("material"),
                    schedule=item.get("schedule"),
                    length=item.get("length"),
                    qty_available=item.get("qty_available"),
                    item_code=item.get("item_code")
                )
                session.add(new_item)

            session.commit()
            return True, f"Ø§Ø³Ù¾ÙˆÙ„ '{spool_id}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª ÙˆÛŒØ±Ø§ÛŒØ´ Ø´Ø¯."
        except Exception as e:
            session.rollback()
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø§Ø³Ù¾ÙˆÙ„: {e}")
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø§Ø³Ù¾ÙˆÙ„: {e}"
        finally:
            session.close()

    def generate_next_spool_id(self) -> str:
        """
        ÛŒÚ© Ø´Ù†Ø§Ø³Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³Ù¾ÙˆÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ø®Ø±ÛŒÙ† Ø´Ù†Ø§Ø³Ù‡ Ù…ÙˆØ¬ÙˆØ¯ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        - Ø¹Ù…Ù„Ú©Ø±Ø¯: ØµØ­ÛŒØ­ Ùˆ Ù‚ÙˆÛŒ. Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² regex Ø¹Ø¯Ø¯ Ø±Ø§ Ø§Ø² Ø§Ù†ØªÙ‡Ø§ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ø´Ù†Ø§Ø³Ù‡ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ ÛŒÚ©ÛŒ Ø¨Ù‡ Ø¢Ù† Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        session = self.get_session()
        try:
            # Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¢Ø®Ø±ÛŒÙ† Ø±Ú©ÙˆØ±Ø¯ØŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ID Ú©Ù‡ Ù‡Ù…ÛŒØ´Ù‡ ØµØ¹ÙˆØ¯ÛŒ Ø§Ø³Øª Ù…Ø±ØªØ¨ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            last_spool = session.query(Spool).order_by(Spool.id.desc()).first()
            if not last_spool:
                return "S001"

            last_id = last_spool.spool_id
            # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ø§Ø¹Ø¯Ø§Ø¯ Ø¯Ø± Ø´Ù†Ø§Ø³Ù‡
            numeric_parts = re.findall(r'\d+', last_id)

            if numeric_parts:
                # Ø¢Ø®Ø±ÛŒÙ† Ø¹Ø¯Ø¯ Ù¾ÛŒØ¯Ø§ Ø´Ø¯Ù‡ Ø±Ø§ ÛŒÚ©ÛŒ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
                next_num = int(numeric_parts[-1]) + 1
            else:
                # Ø§Ú¯Ø± Ù‡ÛŒÚ† Ø¹Ø¯Ø¯ÛŒ Ø¯Ø± Ø´Ù†Ø§Ø³Ù‡ Ù†Ø¨ÙˆØ¯ØŒ Ø§Ø² ID Ø®ÙˆØ¯ Ø±Ú©ÙˆØ±Ø¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                next_num = last_spool.id + 1

            # ÙØ±Ù…Øªâ€ŒØ¯Ù‡ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ø³Ù‡ Ø±Ù‚Ù…ÛŒ (e.g., S001, S012, S123)
            return f"S{next_num:03d}"
        except Exception as e:
            logging.error(f"Error generating next spool ID: {e}")
            return f"S_ERR_{datetime.now().microsecond}"
        finally:
            session.close()

    def get_spool_by_id(self, spool_id: str):
        """
        ÛŒÚ© Ø§Ø³Ù¾ÙˆÙ„ Ø±Ø§ Ø¨Ù‡ Ù‡Ù…Ø±Ø§Ù‡ ØªÙ…Ø§Ù… Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒ Ù…Ø±ØªØ¨Ø· Ø¨Ø§ Ø¢Ù† Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
        - Ø¹Ù…Ù„Ú©Ø±Ø¯: ØµØ­ÛŒØ­. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² joinedload Ø¨Ø§Ø¹Ø« Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ Ù‡Ù…Ø±Ø§Ù‡ Ø¨Ø§ Ø®ÙˆØ¯ Ø§Ø³Ù¾ÙˆÙ„ Ø¯Ø± ÛŒÚ© Ú©ÙˆØ¦Ø±ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù‡ Ø´ÙˆÙ†Ø¯ Ú©Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡ Ø§Ø³Øª.
        """
        session = self.get_session()
        try:
            spool = session.query(Spool).filter(Spool.spool_id == spool_id).options(joinedload(Spool.items)).first()
            return spool
        finally:
            session.close()  # Ø¨Ø³ØªÙ† Ø³Ø´Ù† Ø¯Ø± Ù‡Ø± ØµÙˆØ±Øª Ø¶Ø±ÙˆØ±ÛŒ Ø§Ø³Øª

    def export_spool_data_to_excel(self, file_path: str) -> Tuple[bool, str]:
        """
        Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯Ø§ÙˆÙ„ Spool, SpoolItem Ùˆ SpoolConsumption Ø±Ø§ Ø¨Ù‡ Ø´ÛŒØªâ€ŒÙ‡Ø§ÛŒ Ù…Ø¬Ø²Ø§ Ø¯Ø± ÛŒÚ© ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„ Ø®Ø±ÙˆØ¬ÛŒ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
        - Ø¹Ù…Ù„Ú©Ø±Ø¯: ØµØ­ÛŒØ­. Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ù¾Ø§Ù†Ø¯Ø§Ø² Ø¨Ø±Ø§ÛŒ Ø®ÙˆØ§Ù†Ø¯Ù† Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² Ú©ÙˆØ¦Ø±ÛŒ Ùˆ Ù†ÙˆØ´ØªÙ† Ø¯Ø± Ø§Ú©Ø³Ù„ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ú©Ø§Ø±Ø¢Ù…Ø¯ Ø§Ø³Øª.
        """
        session = self.get_session()
        try:
            tables_to_export = {
                "Spools": Spool,
                "SpoolItems": SpoolItem,
                "SpoolConsumptions": SpoolConsumption
            }
            with pd.ExcelWriter(file_path, engine='openpyxl') as writer:
                for sheet_name, model_class in tables_to_export.items():
                    query = session.query(model_class)
                    df = pd.read_sql(query.statement, session.bind)
                    df.to_excel(writer, sheet_name=sheet_name, index=False)

            self.log_activity("system", "EXPORT_TO_EXCEL", f"Spool data exported to {file_path}")
            return True, f"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø± ÙØ§ÛŒÙ„ {file_path} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯."
        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ú¯Ø±ÙØªÙ† Ø§Ú©Ø³Ù„: {e}")
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ø§Ú©Ø³Ù„: {e}"
        finally:
            session.close()

    def get_all_spool_ids(self) -> list[str]:
        """
        Ù„ÛŒØ³ØªÛŒ Ø§Ø² ØªÙ…Ø§Ù… Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø³Ù¾ÙˆÙ„ Ù…ÙˆØ¬ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
        - Ø¹Ù…Ù„Ú©Ø±Ø¯: ØµØ­ÛŒØ­. Ú©ÙˆØ¦Ø±ÛŒ Ø³Ø§Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† ØªÙ…Ø§Ù… Ø´Ù†Ø§Ø³Ù‡â€ŒÙ‡Ø§ Ùˆ ØªØ¨Ø¯ÛŒÙ„ Ù†ØªÛŒØ¬Ù‡ Ø¨Ù‡ Ù„ÛŒØ³Øª Ø±Ø´ØªÙ‡â€ŒÙ‡Ø§.
        """
        session = self.get_session()
        try:
            results = session.query(Spool.spool_id).order_by(Spool.spool_id).all()
            return [item[0] for item in results]
        except Exception as e:
            logging.error(f"Error fetching all spool IDs: {e}")
            return []
        finally:
            session.close()

    # --------------------------------------------------------------------
    # Ù…ØªØ¯Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø§Ù¾Ø¯ÛŒØª CSV
    # --------------------------------------------------------------------

    def get_or_create_project(self, session, project_name: str) -> Project:
        """ÛŒÚ© Ù¾Ø±ÙˆÚ˜Ù‡ Ø±Ø§ Ø¨Ø§ Ù†Ø§Ù… Ø¢Ù† Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù‡ ÛŒØ§ Ø¯Ø± ØµÙˆØ±Øª Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ØŒ Ø¢Ù† Ø±Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        project = session.query(Project).filter(Project.name == project_name).first()
        if not project:
            self.log_activity("system", "CREATE_PROJECT",
                              f"Ù¾Ø±ÙˆÚ˜Ù‡ '{project_name}' Ø¨Ù‡ ØµÙˆØ±Øª Ø®ÙˆØ¯Ú©Ø§Ø± Ø­ÛŒÙ† Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡ Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯.", session)
            project = Project(name=project_name)
            session.add(project)
            session.flush()  # Ø¨Ø±Ø§ÛŒ Ú¯Ø±ÙØªÙ† Ø´Ù†Ø§Ø³Ù‡ Ù¾Ø±ÙˆÚ˜Ù‡ Ø¬Ø¯ÛŒØ¯
        return project

    def update_project_mto_from_csv(self, project_name: str, mto_file_path: str) -> Tuple[bool, str]:
        """
        --- CHANGE: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨ÛŒØ´ØªØ± ---
        """
        # --- CHANGE: ØªØ¹Ø±ÛŒÙ Ù†Ú¯Ø§Ø´Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ Ùˆ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ---
        REQUIRED_DB_COLS = {'line_no', 'description'}
        MTO_COLUMN_MAP = {
            'UNIT': 'unit', 'LINENO': 'line_no', 'CLASS': 'item_class', 'TYPE': 'item_type',
            'DESCRIPTION': 'description', 'ITEMCODE': 'item_code', 'MAT': 'material_code',
            'P1BOREIN': 'p1_bore_in', 'P2BOREIN': 'p2_bore_in', 'P3BOREIN': 'p3_bore_in',
            'LENGTHM': 'length_m', 'QUANTITY': 'quantity', 'JOINT': 'joint', 'INCHDIA': 'inch_dia'
        }

        session = self.get_session()
        try:
            with session.begin():
                self.log_activity("system", "MTO_UPDATE_START", f"Ø´Ø±ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª MTO Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ '{project_name}'.", session)

                project = self.get_or_create_project(session, project_name)
                project_id = project.id

                # Ø®ÙˆØ§Ù†Ø¯Ù† Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ DataFrame Ø¨Ø§ ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯
                mto_df_raw = pd.read_csv(mto_file_path, dtype=str).fillna('')
                mto_df = self._normalize_and_rename_df(
                    mto_df_raw, MTO_COLUMN_MAP, REQUIRED_DB_COLS, os.path.basename(mto_file_path)
                )
                mto_df['project_id'] = project_id

                # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
                for col in ['p1_bore_in', 'p2_bore_in', 'p3_bore_in', 'length_m', 'quantity', 'joint', 'inch_dia']:
                     if col in mto_df.columns:
                        mto_df[col] = pd.to_numeric(mto_df[col], errors='coerce')


                # Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
                mto_item_ids_to_delete = session.query(MTOItem.id).filter(MTOItem.project_id == project_id).scalar_subquery()
                session.query(MTOConsumption).filter(MTOConsumption.mto_item_id.in_(mto_item_ids_to_delete)).delete(synchronize_session=False)
                session.query(MTOProgress).filter(MTOProgress.project_id == project_id).delete(synchronize_session=False)
                session.query(MTOItem).filter(MTOItem.project_id == project_id).delete(synchronize_session=False)
                session.flush()

                # Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
                mto_records = mto_df.to_dict(orient='records')
                if mto_records:
                    session.bulk_insert_mappings(MTOItem, mto_records)

            self.log_activity("system", "MTO_UPDATE_SUCCESS", f"{len(mto_df)} Ø¢ÛŒØªÙ… MTO Ø¨Ø±Ø§ÛŒ '{project_name}' Ø¢Ù¾Ø¯ÛŒØª Ø´Ø¯.")
            return True, f"âœ” Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ MTO Ø¨Ø±Ø§ÛŒ Ù¾Ø±ÙˆÚ˜Ù‡ '{project_name}' Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø´Ø¯Ù†Ø¯."

        except (ValueError, KeyError, FileNotFoundError) as e:
            session.rollback()
            logging.error(f"Ø´Ú©Ø³Øª Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª MTO Ø¨Ø±Ø§ÛŒ {project_name}: {e}")
            return False, f"Ø®Ø·Ø§ Ø¯Ø± ÙØ§ÛŒÙ„ MTO Ù¾Ø±ÙˆÚ˜Ù‡ '{project_name}': {e}"
        except Exception as e:
            session.rollback()
            logging.error(f"An unexpected error occurred during MTO update for {project_name}: {e}")
            return False, f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª MTO Ù¾Ø±ÙˆÚ˜Ù‡ '{project_name}': {e}"
        finally:
            session.close()

    def process_selected_csv_files(self, file_paths: List[str]) -> Tuple[bool, str]:
        """
        --- NEW: ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ CSV Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ ---
        ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¹Ù…Ù„ÛŒØ§Øª Ø¢Ù¾Ø¯ÛŒØª Ù…Ø±Ø¨ÙˆØ·Ù‡ (MTO ÛŒØ§ Spool) Ø±Ø§ Ø§Ø¬Ø±Ø§ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        try:
            # Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
            mto_files = {}
            spool_file = None
            spool_items_file = None

            for path in file_paths:
                filename = os.path.basename(path)
                if filename.upper().startswith("MTO-") and filename.upper().endswith(".CSV"):
                    project_name = filename.replace("MTO-", "").replace(".csv", "")
                    mto_files[project_name] = path
                elif filename.upper() == "SPOOLS.CSV":
                    spool_file = path
                elif filename.upper() == "SPOOLITEMS.CSV":
                    spool_items_file = path

            # Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø±Ø§ÛŒØ· Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù†ÙˆØ¹ Ø¢Ù¾Ø¯ÛŒØª
            can_update_spool = spool_file and spool_items_file
            can_update_mto = bool(mto_files)

            if not can_update_spool and not can_update_mto:
                return False, "Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ù†Ø´Ø¯.\nØ¨Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª MTOØŒ Ù†Ø§Ù… ÙØ§ÛŒÙ„ Ø¨Ø§ÛŒØ¯ `MTO-ProjectName.csv` Ø¨Ø§Ø´Ø¯.\nØ¨Ø±Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØª SpoolØŒ Ù‡Ø± Ø¯Ùˆ ÙØ§ÛŒÙ„ `Spools.csv` Ùˆ `SpoolItems.csv` Ø¨Ø§ÛŒØ¯ Ø§Ù†ØªØ®Ø§Ø¨ Ø´ÙˆÙ†Ø¯."

            summary_log = []

            # Û±. Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Spool (Ø§Ú¯Ø± Ù‡Ø± Ø¯Ùˆ ÙØ§ÛŒÙ„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯)
            # Ø§ÛŒÙ† Ø¹Ù…Ù„ÛŒØ§Øª Ø§ÙˆÙ„ Ø§Ù†Ø¬Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯ Ú†ÙˆÙ† Ù…Ù…Ú©Ù† Ø§Ø³Øª MTO Ø¨Ù‡ Ø¢Ù† ÙˆØ§Ø¨Ø³ØªÙ‡ Ø¨Ø§Ø´Ø¯.
            if can_update_spool:
                logging.info("Processing Spool files...")
                success, message = self.replace_all_spool_data(spool_file, spool_items_file)
                if not success:
                    # Ø§Ú¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ø§Ø³Ù¾ÙˆÙ„ Ø´Ú©Ø³Øª Ø¨Ø®ÙˆØ±Ø¯ØŒ Ú©Ù„ Ø¹Ù…Ù„ÛŒØ§Øª Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯
                    return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Spool: {message}"
                summary_log.append(message)

            # Û². Ø¢Ù¾Ø¯ÛŒØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ MTO (Ø¨Ø±Ø§ÛŒ Ù‡Ø± ÙØ§ÛŒÙ„ MTO Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø¬Ø²Ø§)
            if can_update_mto:
                for project_name, mto_path in sorted(mto_files.items()):
                    logging.info(f"Processing MTO file for project '{project_name}'...")
                    success, message = self.update_project_mto_from_csv(project_name, mto_path)
                    if not success:
                        # Ø§Ú¯Ø± Ø¢Ù¾Ø¯ÛŒØª ÛŒÚ© Ù¾Ø±ÙˆÚ˜Ù‡ Ø´Ú©Ø³Øª Ø¨Ø®ÙˆØ±Ø¯ØŒ Ú©Ù„ Ø¹Ù…Ù„ÛŒØ§Øª Ù…ØªÙˆÙ‚Ù Ù…ÛŒâ€ŒØ´ÙˆØ¯
                        error_msg = f"Ø®Ø·Ø§ Ø¯Ø± Ø¢Ù¾Ø¯ÛŒØª Ù¾Ø±ÙˆÚ˜Ù‡ '{project_name}': {message}. Ø¹Ù…Ù„ÛŒØ§Øª Ù…ØªÙˆÙ‚Ù Ø´Ø¯."
                        return False, error_msg
                    summary_log.append(message)

            return True, "\n".join(summary_log)

        except Exception as e:
            import traceback
            logging.error(f"An unexpected error occurred in process_selected_csv_files: {traceback.format_exc()}")
            return False, f"ÛŒÚ© Ø®Ø·Ø§ÛŒ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ù†Ø´Ø¯Ù‡ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø±Ø® Ø¯Ø§Ø¯: {e}"

    def replace_all_spool_data(self, spool_file_path: str, spool_items_file_path: str) -> Tuple[bool, str]:
        """
        --- CHANGE: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² ØªØ§Ø¨Ø¹ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ÛŒ Ø¨ÛŒØ´ØªØ± ---
        """
        # --- CHANGE: ØªØ¹Ø±ÛŒÙ Ù†Ú¯Ø§Ø´Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ Ùˆ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ---
        REQUIRED_SPOOL_DB_COLS = {"spool_id"}
        REQUIRED_SPOOL_ITEM_DB_COLS = {"spool_id_str", "component_type"}

        SPOOL_COLUMN_MAP = {
            "SPOOLID": "spool_id", "ROWNO": "row_no", "LOCATION": "location", "COMMAND": "command"
        }
        SPOOL_ITEM_COLUMN_MAP = {
            "SPOOLID": "spool_id_str", "COMPONENTTYPE": "component_type", "CLASSANGLE": "class_angle",
            "P1BORE": "p1_bore", "P2BORE": "p2_bore", "MATERIAL": "material", "SCHEDULE": "schedule",
            "THICKNESS": "thickness", "LENGTH": "length", "QTYAVAILABLE": "qty_available", "ITEMCODE": "item_code"
        }
        session = self.get_session()
        try:
            with session.begin():
                # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Spools.csv
                spools_df_raw = pd.read_csv(spool_file_path, dtype=str).fillna('')
                spools_df = self._normalize_and_rename_df(
                    spools_df_raw, SPOOL_COLUMN_MAP, REQUIRED_SPOOL_DB_COLS, os.path.basename(spool_file_path)
                )
                spools_df['spool_id'] = spools_df['spool_id'].str.strip().str.upper()

                # Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ SpoolItems.csv
                spool_items_df_raw = pd.read_csv(spool_items_file_path, dtype=str).fillna('')
                spool_items_df = self._normalize_and_rename_df(
                    spool_items_df_raw, SPOOL_ITEM_COLUMN_MAP, REQUIRED_SPOOL_ITEM_DB_COLS, os.path.basename(spool_items_file_path)
                )
                spool_items_df['spool_id_str'] = spool_items_df['spool_id_str'].str.strip().str.upper()

                # Ø­Ø°Ù Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù‚Ø¯ÛŒÙ…ÛŒ (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
                session.query(SpoolConsumption).delete(synchronize_session=False)
                session.query(SpoolItem).delete(synchronize_session=False)
                session.query(Spool).delete(synchronize_session=False)
                session.flush()

                # Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Spools Ùˆ Ø³Ø§Ø®Øª Ù†Ú¯Ø§Ø´Øª (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
                spool_records = spools_df.to_dict(orient="records")
                if spool_records:
                    session.bulk_insert_mappings(Spool, spool_records)
                session.flush()

                spool_id_map = {spool.spool_id: spool.id for spool in session.query(Spool.id, Spool.spool_id).all()}

                # Ø¯Ø±Ø¬ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ SpoolItems (Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±)
                spool_items_df["spool_id_fk"] = spool_items_df["spool_id_str"].map(spool_id_map)
                spool_items_df.dropna(subset=["spool_id_fk"], inplace=True)
                spool_items_df["spool_id_fk"] = spool_items_df["spool_id_fk"].astype(int)

                # ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¹Ø¯Ø¯ÛŒ
                for col in ["class_angle", "p1_bore", "p2_bore", "thickness", "length", "qty_available"]:
                     if col in spool_items_df.columns:
                        spool_items_df[col] = pd.to_numeric(spool_items_df[col], errors='coerce')

                item_records = spool_items_df.drop(columns=["spool_id_str"]).to_dict(orient="records")
                if item_records:
                    session.bulk_insert_mappings(SpoolItem, item_records)

            self.log_activity("system", "SPOOL_UPDATE_SUCCESS", f"{len(spools_df)} Ø§Ø³Ù¾ÙˆÙ„ Ùˆ {len(spool_items_df)} Ø¢ÛŒØªÙ… Ø§Ø³Ù¾ÙˆÙ„ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´Ø¯Ù†Ø¯.")
            return True, "âœ” Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Spool Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ù‡ ØµÙˆØ±Øª Ú©Ø§Ù…Ù„ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´Ø¯Ù†Ø¯."

        except (ValueError, KeyError, FileNotFoundError) as e:
            return False, f"Ø®Ø·Ø§ Ø¯Ø± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Spool: {e}"
        except Exception as e:
            return False, f"Ø®Ø·Ø§ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¯Ø± Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†ÛŒ Spool: {e}. (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù…ØµØ±ÙÛŒ Ù…Ø§Ù†Ø¹ Ø­Ø°Ù Ø´Ø¯Ù‡ Ø¨Ø§Ø´Ù†Ø¯)"
        finally:
            session.close()

    def _validate_and_normalize_df(self, df: pd.DataFrame, required_columns: set, file_name: str) -> pd.DataFrame:
        """
        Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ DataFrame Ø±Ø§ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ø¯Ù‡ Ùˆ ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """
        # Û±. ØªÙ…Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯ ØªØ¨Ø¯ÛŒÙ„ Ú©Ø±Ø¯Ù‡ Ùˆ ÙØ¶Ø§Ù‡Ø§ÛŒ Ø®Ø§Ù„ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ Ø±Ø§ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
        original_columns = df.columns
        df.columns = [str(col).strip().upper() for col in original_columns]

        # Û². Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ú©Ù‡ Ø¢ÛŒØ§ ØªÙ…Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ù†Ø¯ ÛŒØ§ Ø®ÛŒØ±
        present_columns = set(df.columns)
        missing_columns = required_columns - present_columns

        # Û³. Ø§Ú¯Ø± Ø³ØªÙˆÙ†ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ ÛŒÚ© Ù¾ÛŒØ§Ù… Ø®Ø·Ø§ÛŒ Ø¯Ù‚ÛŒÙ‚ Ù†Ù…Ø§ÛŒØ´ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
        if missing_columns:
            missing_str = ", ".join(sorted(list(missing_columns)))
            raise ValueError(f"ÙØ§ÛŒÙ„ '{file_name}' Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø²ÛŒØ± Ø±Ø§ Ù†Ø¯Ø§Ø±Ø¯: {missing_str}")

        return df

    def _normalize_and_rename_df(self, df: pd.DataFrame, column_map: dict, required_db_cols: set,
                                 file_name: str) -> pd.DataFrame:
        """
        --- CHANGE: Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† .copy() Ø¯Ø± Ø§Ù†ØªÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ù‡Ø´Ø¯Ø§Ø± ---
        1. Ù†Ø§Ù… Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÙØ§ÛŒÙ„ CSV Ø±Ø§ Ù†Ø±Ù…Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (Ø­Ø°Ù Ø¹Ù„Ø§Ø¦Ù…ØŒ ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ø­Ø±ÙˆÙ Ø¨Ø²Ø±Ú¯).
        2. Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø§ Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ column_map Ø¨Ù‡ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø³ØªÙˆÙ† Ø¯ÛŒØªØ§Ø¨ÛŒØ³ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        3. ÙˆØ¬ÙˆØ¯ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø±Ø§ Ù¾Ø³ Ø§Ø² ØªØºÛŒÛŒØ± Ù†Ø§Ù…ØŒ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        """

        def normalize_header(name: str) -> str:
            """ÛŒÚ© Ù†Ø§Ù… Ø³ØªÙˆÙ† Ø±Ø§ Ú¯Ø±ÙØªÙ‡ Ùˆ Ø¢Ù† Ø±Ø§ Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
            return re.sub(r'[^A-Z0-9]', '', str(name).upper())

        rename_map = {}
        found_db_cols = set()

        for original_col in df.columns:
            normalized_col = normalize_header(original_col)
            if normalized_col in column_map:
                db_col = column_map[normalized_col]
                rename_map[original_col] = db_col
                found_db_cols.add(db_col)

        missing_cols = required_db_cols - found_db_cols
        if missing_cols:
            missing_str = ", ".join(sorted(list(missing_cols)))
            raise ValueError(f"ÙØ§ÛŒÙ„ '{file_name}' Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø²ÛŒØ± Ø±Ø§ Ù†Ø¯Ø§Ø±Ø¯: {missing_str}")

        df.rename(columns=rename_map, inplace=True)

        final_cols = [col for col in df.columns if col in found_db_cols]

        # --- CHANGE: Ø¨Ø§ Ø§ÙØ²ÙˆØ¯Ù† .copy()ØŒ ÛŒÚ© DataFrame Ø¬Ø¯ÛŒØ¯ Ùˆ Ù…Ø³ØªÙ‚Ù„ Ø³Ø§Ø®ØªÙ‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯ ---
        return df[final_cols].copy()

    # --------------------------------------------------------------------
    # Ù…ØªØ¯Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§ÛŒØ²Ùˆ Ù‡Ø§
    # --------------------------------------------------------------------

    def _normalize_line_key(self, text: str) -> str:
        """(Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±) Ú©Ù„ÛŒØ¯ Ù†Ø±Ù…Ø§Ù„â€ŒØ´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø±Ø§ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        if not text: return ""
        return re.sub(r'[^A-Z0-9]+', '', text.upper())

    def _extract_prefix_key(self, text: str) -> str:
        """(Ø¨Ø¯ÙˆÙ† ØªØºÛŒÛŒØ±) Ú©Ù„ÛŒØ¯ Ù¾ÛŒØ´ÙˆÙ†Ø¯ (ØªØ§ Ø§ÙˆÙ„ÛŒÙ† Û¶ Ø±Ù‚Ù…) Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        norm = self._normalize_line_key(text)
        m = re.search(r'(\d{6})', norm)
        return norm[:m.end(1)] if m else norm

    def find_iso_files(self, line_text: str, limit: int = 200) -> list[str]:
        """
        (Ù†Ø³Ø®Ù‡ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø´Ø¨Ø§Ù‡Øª)
        Ø§Ø¨ØªØ¯Ø§ Ø¨Ø§ ÛŒÚ© Ú©ÙˆØ¦Ø±ÛŒ Ø³Ø±ÛŒØ¹ Ú©Ø§Ù†Ø¯ÛŒØ¯Ø§Ù‡Ø§ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù‡ØŒ Ø³Ù¾Ø³ Ø¢Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø´Ø¨Ø§Ù‡Øª
        Ø¨Ù‡ ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø± Ù…Ø±ØªØ¨ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªØ§ÛŒØ¬ Ø¯Ø± Ø§Ø¨ØªØ¯Ø§ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆÙ†Ø¯.
        """
        from models import IsoFileIndex
        session = self.get_session()
        try:
            norm_input = self._normalize_line_key(line_text)
            if not norm_input:
                return []

            # Ù…Ø±Ø­Ù„Ù‡ Û±: ÙˆØ§Ú©Ø´ÛŒ Ú©Ø§Ù†Ø¯ÛŒØ¯Ø§Ù‡Ø§ Ø§Ø² Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø¨Ø§ ÛŒÚ© Ú©ÙˆØ¦Ø±ÛŒ Ù…Ù†Ø¹Ø·Ùâ€ŒØªØ±
            # Ù…Ø§ Ø¨Ù‡ Ø¬Ø§ÛŒ prefixØŒ Ø§Ø² Ø®ÙˆØ¯ norm_input Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø§Ù†Ø¹Ø·Ø§Ùâ€ŒÙ¾Ø°ÛŒØ±ØªØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ….
            # Ø§ÛŒÙ† Ú©Ø§Ø± Ù†ØªØ§ÛŒØ¬ Ù…Ø±ØªØ¨Ø· Ø¨ÛŒØ´ØªØ±ÛŒ Ø±Ø§ Ø¯Ø± Ù…Ø±Ø­Ù„Ù‡ Ø§ÙˆÙ„ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯.
            search_term = f"%{norm_input}%"
            candidate_records = session.query(
                IsoFileIndex.file_path,
                IsoFileIndex.normalized_name
            ).filter(
                IsoFileIndex.normalized_name.like(search_term)
            ).limit(limit * 2).all()  # Ú©Ù…ÛŒ Ø¨ÛŒØ´ØªØ± Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø² Ù…ÛŒâ€ŒØ®ÙˆØ§Ù†ÛŒÙ… ØªØ§ ÙØ¶Ø§ÛŒ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´ÛŒÙ…

            if not candidate_records:
                return []

            # Ù…Ø±Ø­Ù„Ù‡ Û²: Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¨Ø§Ù‡Øª Ùˆ Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¯Ø± Ù¾Ø§ÛŒØªÙˆÙ†
            scored_results = []
            for file_path, normalized_name in candidate_records:
                # SequenceMatcher Ø´Ø¨Ø§Ù‡Øª Ø¨ÛŒÙ† Ø¯Ùˆ Ø±Ø´ØªÙ‡ Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
                ratio = difflib.SequenceMatcher(None, norm_input, normalized_name).ratio()
                # Ø§Ú¯Ø± ÙˆØ±ÙˆØ¯ÛŒ Ú©Ø§Ø±Ø¨Ø± Ø¯Ø± Ù†Ø§Ù… ÙØ§ÛŒÙ„ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ø¯ØŒ ÛŒÚ© Ø§Ù…ØªÛŒØ§Ø² Ø§Ø¶Ø§ÙÙ‡ Ù…ÛŒâ€ŒØ¯Ù‡ÛŒÙ…
                if norm_input in normalized_name:
                    ratio += 0.1

                scored_results.append((ratio, file_path))

            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ù†ØªØ§ÛŒØ¬ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ù…ØªÛŒØ§Ø² Ø´Ø¨Ø§Ù‡Øª (Ø§Ø² Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø¨Ù‡ Ú©Ù…ØªØ±ÛŒÙ†)
            scored_results.sort(key=lambda x: x[0], reverse=True)

            # Ø¨Ø±Ú¯Ø±Ø¯Ø§Ù†Ø¯Ù† Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ limit
            return [file_path for ratio, file_path in scored_results[:limit]]

        except Exception as e:
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¬Ø³ØªØ¬ÙˆÛŒ Ù‡ÙˆØ´Ù…Ù†Ø¯ ÙØ§ÛŒÙ„ ISO: {e}")
            return []
        finally:
            session.close()

    def upsert_iso_index_entry(self, file_path: str):
        """ÛŒÚ© ÙØ§ÛŒÙ„ Ø±Ø§ Ø¯Ø± Ø¬Ø¯ÙˆÙ„ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¯Ø±Ø¬ ÛŒØ§ Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ (UPSERT)."""
        session = self.get_session()
        try:
            if not os.path.exists(file_path):
                self.remove_iso_index_entry(file_path)
                return

            filename = os.path.basename(file_path)
            normalized_name = self._normalize_line_key(filename)
            prefix_key = self._extract_prefix_key(filename)
            last_modified = datetime.fromtimestamp(os.path.getmtime(file_path))

            # Ø§Ø¨ØªØ¯Ø§ Ø³Ø¹ÛŒ Ú©Ù† Ø±Ú©ÙˆØ±Ø¯ Ù…ÙˆØ¬ÙˆØ¯ Ø±Ø§ Ù¾ÛŒØ¯Ø§ Ú©Ù†ÛŒ
            record = session.query(IsoFileIndex).filter(IsoFileIndex.file_path == file_path).first()
            if record:
                # Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªØŒ Ø¢Ù¾Ø¯ÛŒØª Ú©Ù†
                record.normalized_name = normalized_name
                record.prefix_key = prefix_key
                record.last_modified = last_modified
            else:
                # Ø§Ú¯Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªØŒ ÛŒÚ©ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø³Ø§Ø²
                record = IsoFileIndex(
                    file_path=file_path,
                    normalized_name=normalized_name,
                    prefix_key=prefix_key,
                    last_modified=last_modified
                )
                session.add(record)
            session.commit()
        except Exception as e:
            session.rollback()
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± upsert_iso_index_entry Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„ {file_path}: {e}")
        finally:
            session.close()

    def remove_iso_index_entry(self, file_path: str):
        """ÛŒÚ© ÙØ§ÛŒÙ„ Ø±Ø§ Ø§Ø² Ø¬Ø¯ÙˆÙ„ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø­Ø°Ù Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        session = self.get_session()
        try:
            session.query(IsoFileIndex).filter(IsoFileIndex.file_path == file_path).delete()
            session.commit()
        except Exception as e:
            session.rollback()
            logging.error(f"Ø®Ø·Ø§ Ø¯Ø± remove_iso_index_entry Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„ {file_path}: {e}")
        finally:
            session.close()

    def rebuild_iso_index_from_scratch(self, base_dir: str, event_handler=None):
        """
        (Ù†Ø³Ø®Ù‡ Ù†Ù‡Ø§ÛŒÛŒ Ùˆ Ù‡ÙˆØ´Ù…Ù†Ø¯)
        Ø§ÛŒÙ†Ø¯Ú©Ø³ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ ISO Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ØºÛŒØ±Ù…Ø®Ø±Ø¨ Ùˆ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        - Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù‚Ø¯ÛŒÙ…ÛŒ Ø±Ø§ Ø­Ø°Ù Ù†Ù…ÛŒâ€ŒÚ©Ù†Ø¯ØŒ Ø¨Ù„Ú©Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ø±Ø§ Ø§Ø¹Ù…Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
        - ÙˆØ¶Ø¹ÛŒØª Ù¾ÛŒØ´Ø±ÙØª Ø±Ø§ Ø§Ø² Ø·Ø±ÛŒÙ‚ Ø³ÛŒÚ¯Ù†Ø§Ù„ Ú¯Ø²Ø§Ø±Ø´ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
# IMPROVE: Add type hints for better IDE support
        """
        session = self.get_session()

        def emit_status(message, level):
            if event_handler and hasattr(event_handler, 'status_updated'):
                event_handler.status_updated.emit(message, level)
            else:
                logging.info(f"[{level.upper()}] {message}")

        def emit_progress(value):
            if event_handler and hasattr(event_handler, 'progress_updated'):
                event_handler.progress_updated.emit(value)

        try:
            emit_status("Ø´Ø±ÙˆØ¹ Ø§Ø³Ú©Ù† ÙØ§ÛŒÙ„â€ŒÙ‡Ø§...", "info")
            emit_progress(0)

            # Ú¯Ø§Ù… Û±: ØªÙ…Ø§Ù… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ PDF Ùˆ DWG Ø±Ø§ Ø§Ø² Ø±ÙˆÛŒ Ø¯ÛŒØ³Ú© Ù¾ÛŒØ¯Ø§ Ú©Ù†
            disk_files_paths = glob.glob(os.path.join(base_dir, "**", "*.pdf"), recursive=True)
            disk_files_paths.extend(glob.glob(os.path.join(base_dir, "**", "*.dwg"), recursive=True))

            if not disk_files_paths:
                emit_status("Ù‡ÛŒÚ† ÙØ§ÛŒÙ„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù¾Ø§Ú© Ø´Ø¯.", "warning")
                with session.begin():
                    session.query(IsoFileIndex).delete()
                emit_progress(100)
                return

            # Ø³Ø§Ø®Øª ÛŒÚ© Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø² ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ø±ÙˆÛŒ Ø¯ÛŒØ³Ú© Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹
            disk_files_map = {}
            for path in disk_files_paths:
                try:
                    disk_files_map[path] = datetime.fromtimestamp(os.path.getmtime(path))
                except (FileNotFoundError, OSError):
                    continue  # ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ù„Ø­Ø¸Ù‡ Ø§Ø³Ú©Ù† Ø­Ø°Ù Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ø±Ø§ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ø¨Ú¯ÛŒØ±

            emit_status("Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…ÙˆØ¬ÙˆØ¯...", "info")

            # Ú¯Ø§Ù… Û²: ØªÙ…Ø§Ù… Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø±Ø§ Ø¨Ø®ÙˆØ§Ù†
            db_files_map = {record.file_path: record.last_modified for record in session.query(IsoFileIndex).all()}

            # Ú¯Ø§Ù… Û³: ØªØºÛŒÛŒØ±Ø§Øª Ø±Ø§ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ú©Ù†
            db_paths = set(db_files_map.keys())
            disk_paths = set(disk_files_map.keys())

            paths_to_add = disk_paths - db_paths
            paths_to_delete = db_paths - disk_paths
            paths_to_check = db_paths.intersection(disk_paths)

            records_to_add = []
            records_to_update = []

            total_ops = len(paths_to_add) + len(paths_to_delete) + len(paths_to_check)
            completed_ops = 0
            last_progress = -1

            # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯
            for path in paths_to_add:
                records_to_add.append({
                    "file_path": path,
                    "normalized_name": self._normalize_line_key(os.path.basename(path)),
                    "prefix_key": self._extract_prefix_key(os.path.basename(path)),
                    "last_modified": disk_files_map[path]
                })
                completed_ops += 1

            # Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¢Ù¾Ø¯ÛŒØªÛŒ
            for path in paths_to_check:
                if disk_files_map[path] != db_files_map[path]:
                    records_to_update.append({
                        "file_path": path,
                        "last_modified": disk_files_map[path]
                    })
                completed_ops += 1
                # --- Ú¯Ø²Ø§Ø±Ø´ Ù¾ÛŒØ´Ø±ÙØª Ø¯Ø± Ø­ÛŒÙ† Ø¹Ù…Ù„ÛŒØ§Øª ---
                progress = int((completed_ops / total_ops) * 100)
                if progress > last_progress:
                    emit_progress(progress)
                    last_progress = progress

            # Ú¯Ø§Ù… Û´: Ø§Ø¹Ù…Ø§Ù„ ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            emit_status("Ø§Ø¹Ù…Ø§Ù„ ØªØºÛŒÛŒØ±Ø§Øª Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³...", "info")
            with session.begin():
                # Ø­Ø°Ù Ú¯Ø±ÙˆÙ‡ÛŒ
                if paths_to_delete:
                    session.query(IsoFileIndex).filter(IsoFileIndex.file_path.in_(paths_to_delete)).delete(
                        synchronize_session=False)

                # Ø§ÙØ²ÙˆØ¯Ù† Ú¯Ø±ÙˆÙ‡ÛŒ
                if records_to_add:
                    session.bulk_insert_mappings(IsoFileIndex, records_to_add)

                # Ø¢Ù¾Ø¯ÛŒØª (Ø§ÛŒÙ† Ø¨Ø®Ø´ Ø¨Ø§ÛŒØ¯ ÛŒÚ©ÛŒ ÛŒÚ©ÛŒ Ø§Ù†Ø¬Ø§Ù… Ø´ÙˆØ¯ ÙˆÙ„ÛŒ Ù‡Ù…Ú†Ù†Ø§Ù† Ø³Ø±ÛŒØ¹ Ø§Ø³Øª)
                for record_data in records_to_update:
                    session.query(IsoFileIndex).filter(IsoFileIndex.file_path == record_data['file_path']).update(
                        {"last_modified": record_data['last_modified']})

            emit_status(
                f"Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù‡Ù…Ú¯Ø§Ù…â€ŒØ³Ø§Ø²ÛŒ Ø´Ø¯. ({len(records_to_add)} Ø¬Ø¯ÛŒØ¯, {len(paths_to_delete)} Ø­Ø°Ù, {len(records_to_update)} Ø¢Ù¾Ø¯ÛŒØª)",
                "success")
            emit_progress(100)

        except Exception as e:
            emit_status(f"Ø®Ø·Ø§ÛŒ Ø¨Ø­Ø±Ø§Ù†ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ø³Ø§Ø²ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³: {e}", "error")
            session.rollback()
        finally:
            session.close()

    def upsert_iso_index_entry(self, file_path):
        session = self.get_session()
        try:
            norm_name = os.path.splitext(os.path.basename(file_path))[0].upper()
            entry = session.query(IsoFileIndex).filter_by(file_path=file_path).first()
            if entry:
                entry.last_modified = datetime.now()
            else:
                entry = IsoFileIndex(
                    file_path=file_path,
                    normalized_name=norm_name,
                    prefix_key=norm_name.split('-')[0] if '-' in norm_name else norm_name,
                    last_modified=datetime.now()
                )
                session.add(entry)
            session.commit()
        finally:
            session.close()

    def remove_iso_index_entry(self, file_path):
        session = self.get_session()
        try:
            session.query(IsoFileIndex).filter_by(file_path=file_path).delete()
            session.commit()
        finally:
            session.close()

    #--------------------------------------------------------------------
    #  --- Ù…ØªØ¯Ù‡Ø§ÛŒ Ø¨Ø±Ø§ÛŒ ÙˆØ§Ú©Ø´ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ÛŒ ---
    #--------------------------------------------------------------------

    def get_all_transactions_for_training(self, group_by_project=False) -> dict | list:
        """ØªÙ…Ø§Ù… MIV Ù‡Ø§ Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª ØªØ±Ø§Ú©Ù†Ø´ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ú¯Ø± Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
        session = self.get_session()
        try:
            query = session.query(
                MIVRecord.project_id,
                MTOConsumption.miv_record_id,
                MTOItem.item_code
            ).join(MTOItem, MTOConsumption.mto_item_id == MTOItem.id) \
             .join(MIVRecord, MTOConsumption.miv_record_id == MIVRecord.id).all()

            if not group_by_project:
                transactions = {}
                for _, miv_id, item_code in query:
                    if miv_id not in transactions:
                        transactions[miv_id] = set()
                    transactions[miv_id].add(item_code)
                return {'global': [list(items) for items in transactions.values()]}

            # Ú¯Ø±ÙˆÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù¾Ø±ÙˆÚ˜Ù‡
            transactions_by_project = {}
            for project_id, miv_id, item_code in query:
                if project_id not in transactions_by_project:
                    transactions_by_project[project_id] = {}
                if miv_id not in transactions_by_project[project_id]:
                    transactions_by_project[project_id][miv_id] = set()
                transactions_by_project[project_id][miv_id].add(item_code)

            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ ÙØ±Ù…Øª Ù†Ù‡Ø§ÛŒÛŒ
            final_grouped_transactions = {
                f'proj_{pid}': [list(items) for items in mivs.values()]
                for pid, mivs in transactions_by_project.items()
            }
            return final_grouped_transactions
        finally:
            session.close()

    def get_consumption_history_df(self) -> pd.DataFrame:
        """ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ù…ØµØ±Ù Ø±Ø§ Ø¨Ù‡ ØµÙˆØ±Øª DataFrame Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ Ú©Ø³Ø±ÛŒ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
        session = self.get_session()
        try:
            query = session.query(
                MTOItem.item_code,
                MTOConsumption.timestamp,
                MTOConsumption.used_qty
            ).join(MTOItem, MTOConsumption.mto_item_id == MTOItem.id)
            return pd.read_sql(query.statement, session.bind)
        finally:
            session.close()

    def get_all_mivs_for_training(self) -> pd.DataFrame:
        """Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø¢Ù…ÙˆØ²Ø´ Ù…Ø¯Ù„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ø¢Ù…Ø§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        session = self.get_session()
        try:
            query = session.query(
                MTOConsumption.used_qty,
                MTOProgress.total_qty,
                MTOConsumption.timestamp
            ).join(MTOItem, MTOConsumption.mto_item_id == MTOItem.id) \
                .join(MTOProgress, MTOConsumption.mto_item_id == MTOProgress.mto_item_id)
            return pd.read_sql(query.statement, session.bind)
        finally:
            session.close()

    def get_optimized_spool_suggestion(self, project_id: int, line_no: str) -> Tuple[bool, str, dict | None]:
        """
        --- NEW: ØªØ§Ø¨Ø¹ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØµØ±Ù Ø§Ø³Ù¾ÙˆÙ„ ---
        Ø¨Ø§ Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø­Ø±ÛŒØµØ§Ù†Ù‡ (Greedy)ØŒ Ø¨Ù‡ØªØ±ÛŒÙ† ØªØ±Ú©ÛŒØ¨ Ø§Ø³Ù¾ÙˆÙ„â€ŒÙ‡Ø§ Ø±Ø§ Ø¨Ø±Ø§ÛŒ ØªØ£Ù…ÛŒÙ† Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ
        ÛŒÚ© Ø®Ø· Ø¨Ø§ Ú©Ù…ØªØ±ÛŒÙ† Ø¶Ø§ÛŒØ¹Ø§Øª Ù…Ù…Ú©Ù† Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ¯Ù‡Ø¯.
        """
        session = self.get_session()
        try:
            # 1. Ú¯Ø±ÙØªÙ† ØªÙ…Ø§Ù… Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡ Ø®Ø·
            needed_items = session.query(MTOProgress).filter(
                MTOProgress.project_id == project_id,
                MTOProgress.line_no == line_no,
                MTOProgress.remaining_qty > 0,
                MTOItem.item_type.ilike('%PIPE%')  # ÙØ¹Ù„Ø§ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù¾Ø§ÛŒÙ¾
            ).join(MTOItem, MTOProgress.mto_item_id == MTOItem.id).all()

            if not needed_items:
                return True, "Ø§ÛŒÙ† Ø®Ø· Ù‡ÛŒÚ† Ù¾Ø§ÛŒÙ¾ Ø¨Ø§Ù‚ÛŒâ€ŒÙ…Ø§Ù†Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ø¯Ø§Ø±Ø¯.", None

            optimization_plan = {"items": [], "summary": ""}
            total_waste = 0

            for mto_item in needed_items:
                needed_length = mto_item.remaining_qty

                mto_details = session.get(MTOItem, mto_item.mto_item_id)

                # 2. Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† ØªÙ…Ø§Ù… Ø§Ø³Ù¾ÙˆÙ„â€ŒÙ‡Ø§ÛŒ Ù¾Ø§ÛŒÙ¾ Ø³Ø§Ø²Ú¯Ø§Ø±
                compatible_spools = self.get_mapped_spool_items(mto_details.item_type, mto_details.p1_bore_in)

                # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø§Ø³Ù¾ÙˆÙ„â€ŒÙ‡Ø§: Ø§ÙˆÙ„ Ø¢Ù†â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø·ÙˆÙ„Ø´Ø§Ù† Ø¨Ù‡ Ù†ÛŒØ§Ø² Ù…Ø§ Ù†Ø²Ø¯ÛŒÚ©â€ŒØªØ± (ÙˆÙ„ÛŒ Ø¨Ø²Ø±Ú¯ØªØ±) Ø§Ø³Øª
                compatible_spools.sort(
                    key=lambda s: (s.length or 0) - needed_length if (s.length or 0) >= needed_length else float('inf'))

                item_plan = {"mto_desc": mto_item.description, "selections": []}

                # 3. Ø§Ù„Ú¯ÙˆØ±ÛŒØªÙ… Ø­Ø±ÛŒØµØ§Ù†Ù‡
                for spool_item in compatible_spools:
                    if needed_length <= 0: break
                    if (spool_item.length or 0) > 0.01:
                        take_qty = min(needed_length, spool_item.length)

                        item_plan["selections"].append({
                            "spool_item_id": spool_item.id,
                            "spool_id": spool_item.spool.spool_id,
                            "used_qty": take_qty
                        })
                        needed_length -= take_qty

                if needed_length > 0.01:
                    return False, f"Ù…ÙˆØ¬ÙˆØ¯ÛŒ Ø§Ø³Ù¾ÙˆÙ„ Ø¨Ø±Ø§ÛŒ '{mto_item.description}' Ú©Ø§ÙÛŒ Ù†ÛŒØ³Øª. {needed_length:.2f} Ù…ØªØ± Ú©Ù…Ø¨ÙˆØ¯ ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯.", None

                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¶Ø§ÛŒØ¹Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¨Ù‡ØªØ±ÛŒÙ† Ø§Ù†ØªØ®Ø§Ø¨ (Ø§ÙˆÙ„ÛŒÙ† Ø§Ø³Ù¾ÙˆÙ„ Ø¯Ø± Ù„ÛŒØ³Øª Ù…Ø±ØªØ¨â€ŒØ´Ø¯Ù‡)
                best_fit_spool = compatible_spools[0] if compatible_spools else None
                if best_fit_spool and best_fit_spool.length > mto_item.remaining_qty:
                    waste = best_fit_spool.length - mto_item.remaining_qty
                    total_waste += waste

                optimization_plan["items"].append(item_plan)

            summary = f"Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ù‡ÛŒÙ†Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø¯. Ú©Ù„ Ø¶Ø§ÛŒØ¹Ø§Øª (Off-cut) ØªØ®Ù…ÛŒÙ†ÛŒ: {total_waste:.2f} Ù…ØªØ±."
            optimization_plan["summary"] = summary

            return True, summary, optimization_plan

        except Exception as e:
            logging.error(f"Error in spool optimization: {e}")
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ: {e}", None
        finally:
            session.close()

    #--------------------------------------------------------------------
    #ï¸ --- Ù…ØªØ¯Ù‡Ø§ÛŒ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ ---
    #--------------------------------------------------------------------

    def get_recommendations(self, item_codes: list[str], project_id: int) -> list[str]:
        """Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯Ù‡Ø§ Ø±Ø§ Ø§Ø² Ù…ÙˆØªÙˆØ± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ø¨Ø±Ø§ÛŒ ÛŒÚ© Ù¾Ø±ÙˆÚ˜Ù‡ Ø®Ø§Øµ Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        group_key = f'proj_{project_id}'
        return self.recommender.recommend(item_codes, group_key=group_key)

    def get_predicted_shortages(self, project_id: int) -> list[dict]:
        """Ù„ÛŒØ³ØªÛŒ Ø§Ø² Ø¢ÛŒØªÙ…â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡ Ø¨Ø§ Ú©Ø³Ø±ÛŒ Ù…ÙˆØ§Ø¬Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯."""
        session = self.get_session()
        try:
            items_with_remaining = session.query(MTOProgress).filter(
                MTOProgress.project_id == project_id,
                MTOProgress.remaining_qty > 0
            ).all()

            predictions = []
            for item in items_with_remaining:
                predicted_date = self.shortage_predictor.predict(
                    item_code=item.item_code,
                    total_required=item.total_qty,
                    current_used=item.used_qty
                )
                if predicted_date:
                    predictions.append({
                        "Item Code": item.item_code,
                        "Description": item.description,
                        "Remaining Qty": item.remaining_qty,
                        "Predicted Shortage Date": predicted_date
                    })

            # Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ: ØªØ§Ø±ÛŒØ®â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ø®Øµ Ø§ÙˆÙ„ØŒ Ø³Ù¾Ø³ "Ø§Ú©Ù†ÙˆÙ†" Ùˆ Ø¯Ø± Ø¢Ø®Ø± "Ù‡Ø±Ú¯Ø²"
            return sorted(predictions, key=lambda x: (
                '2' if x['Predicted Shortage Date'].startswith('Ø§') else (
                    '3' if x['Predicted Shortage Date'].startswith('Ù‡') else '1'),
                x['Predicted Shortage Date']
            ))
        finally:
            session.close()

    def check_for_anomaly(self, consumption_data: dict):
        """ÛŒÚ© Ø±Ú©ÙˆØ±Ø¯ Ù…ØµØ±Ù Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ù†Ø§Ù‡Ù†Ø¬Ø§Ø±ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
        df = pd.DataFrame([consumption_data])
        is_anomaly = self.anomaly_detector.predict(df)
        if is_anomaly:
            # Ø§Ú¯Ø± Ù†Ø§Ù‡Ù†Ø¬Ø§Ø± Ø¨ÙˆØ¯ØŒ ÛŒÚ© Ù„Ø§Ú¯ ÙˆÛŒÚ˜Ù‡ Ø«Ø¨Øª Ú©Ù†
            details = f"Anomaly Detected! Used: {consumption_data['used_qty']}, Total: {consumption_data['total_qty']}, Time: {consumption_data['timestamp']}"
            self.log_activity(user="AI_SYSTEM", action="ANOMALY_DETECTED", details=details)


# Last modified: 2025-11-17 09:09:32

# Updated: 2025-11-30 07:20:05
